{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Spark Tutorial. Part 1 - Learning Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface\n",
    "\n",
    "Notebooks in this course consist of theoretical information, practical examples and excercises. Exercises are cells with Python code, in which some pieces of code are missing. Your task is to fill these cells in and then pass the filled notebook to the verification system. Also, there is a test cell after each exercise, that can help you check the correctness of your code.\n",
    "\n",
    "If you have any complaints or suggestions, concact us: [info@datascience-school.com](mailto:info@datascience-school.com)\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  An introduction to using Apache Spark with the PySpark API running in the browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Apache Spark](http://spark.apache.org/) is a cluster computing platform designed to be fast and general purpose.\n",
    "\n",
    "Every Spark application contains a driver program that launches parallel operations on a cluster. Driver programs access Spark through a [`SparkContext`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext) object, which represents a connection to a computing cluster.\n",
    "\n",
    "In order to use Spark and its API, we create an instance of the `SparkContext` class, that is usually named `sc`. The good news is that it is automatically included into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# export PYSPARK_PYTHON=/usr/bin/python3\n",
    "# export PYSPARK_DRIVER_PYTHON=ipython3\n",
    "# export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/home/dmitriy/.local/lib/python3.6/site-packages\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3.6\"\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/usr/local/spark/spark-2.4.0-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"First App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.context.SparkContext'>\n"
     ]
    }
   ],
   "source": [
    "# Display the type of the Spark Context sc.\n",
    "print (type(sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use Python's [dir()](https://docs.python.org/2/library/functions.html?highlight=dir#dir) function to get a list of all the attributes and methods accessible through the `sc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_repr_html_', '_serialize_to_jvm', '_temp_dir', '_unbatched_serializer', 'accumulator', 'addFile', 'addPyFile', 'appName', 'applicationId', 'binaryFiles']\n"
     ]
    }
   ],
   "source": [
    "# List sc's attributes.\n",
    "print (dir(sc)[50:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "An RDD in Spark is simply an immutable distributed collection of objects.\n",
    "- *Immutability* means that once RDD is created, it can not be changed.\n",
    "- *Distribution* means that different unique pairs of an RDD are stored on a different partitions.\n",
    "\n",
    "Each RDD is splitted into multiple partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and Actions\n",
    "\n",
    "Since RDD is immutable, there are two ways to create a new one:\n",
    "\n",
    "- By loading an external dataset.\n",
    "- By running transformation on a pre-existing RDD.\n",
    "\n",
    "**Transformations** are lazy operations on a RDD that create one or many new RDDs.\n",
    "\n",
    "**Actions** are RDD operations that produce non-RDD values. They compute a result based on an RDD, and either return it to the driver program or save it to an external storage system (e.g. HDFS). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your first RDD   \n",
    "First, we generate dummy data by creating a list of numbers from 1 to 4561 with a step of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = range(1, 4561, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the size of the list and print its first and last elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size is 2280\n",
      "The elements are [1...4559]\n"
     ]
    }
   ],
   "source": [
    "print (\"The size is {}\".format(len(dummy_data)))\n",
    "print (\"The elements are [{}...{}]\".format(dummy_data[0], dummy_data[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an RDD, we use the [`sc.parallelize()`](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.SparkContext.parallelize) function, which tells Spark to create a new set of input data based on data that is passed in as an argument.\n",
    "\n",
    "There are many different types of RDDs. The base implementation of RDD is [`pyspark.RDD`](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD) and other RDDs inherit from it, so they have the same APIs and are functionally identical.   \n",
    "\n",
    "*Note: After creating RDDs, we can track them in the \"Storage\" tab of the web UI. You may notice that new datasets are not listed there until Spark returns the result of the action being performed. This feature of Spark is called \"lazy evaluation\". It allows Spark to avoid performing unnecessary calculations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(4) Learning about RDDs PythonRDD[1] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []'\n"
     ]
    }
   ],
   "source": [
    "# Parallelize data using 4 partitions.\n",
    "# This operation is a transformation of data into an RDD.\n",
    "# Spark uses lazy evaluation, so no Spark workers will be running here.\n",
    "dummy_rdd = sc.parallelize(dummy_data, 4)\n",
    "\n",
    "# We can name each newly created RDD using the setName() method.\n",
    "dummy_rdd.setName('Learning about RDDs')\n",
    "\n",
    "# Let's view the lineage (the sequence of transformations) of the RDD using toDebugString().\n",
    "print (dummy_rdd.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "One of the most common transformation is the [`map()`](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.map) function. It applies a function to each element of the dataset, and returns resulting dataset of the same length.\n",
    "\n",
    "Let's use `map()` to increment each value in the previously created **`dummy_rdd`** dataset.\n",
    "\n",
    "Remember that most of transformations require a function as an argument. In the following code section we will pass a lambda function to the `map()`. Labmda functions are preferred over named fuctions (defined with `def`) when their body is relatively small and simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
     ]
    }
   ],
   "source": [
    "# Lambda function can be assigned to a variable to make it reusable:\n",
    "increment = lambda x: x + 1\n",
    "\n",
    "# Compare to named function:\n",
    "def increment (x):\n",
    "    return x + 1\n",
    "\n",
    "# These two transformations are identical\n",
    "dummy_incremented = dummy_rdd.map(lambda x: x + 1)\n",
    "dummy_incremented = dummy_rdd.map(increment)\n",
    "\n",
    "# We can't directly see the content of an RDD, because\n",
    "# no transformations are performed on it untill an action is called.\n",
    "print (dummy_incremented.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Actions  \n",
    "To see the resulting list we can use the [`collect()`](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.collect) function.\n",
    "\n",
    "The `collect()` function is the first action operation we have encountered. It returns all the elements of the dataset as an array to the driver program.\n",
    "\n",
    "All actions in Spark cause the transformations applied to an RDD to be computed. Therefore in our example workers will be launched to perform the `parallelize`, `map`, and `collect` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "collected_data = dummy_incremented.collect()\n",
    "\n",
    "# We incremented the initial dataset earlier, so its first element is 2\n",
    "print (collected_data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`takeSample()`](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.takeSample) action is used to retrieve a random sample of data from the dataset. The first argument tells whether the sample can include the same elements, and the second argument specifies the length of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[342, 2408, 1046, 174, 2592, 2166, 232, 866, 58, 3892]\n"
     ]
    }
   ],
   "source": [
    "# Get 10 random elements from the RDD. Elements can be repeated.\n",
    "# Note: returned result is NOT an RDD, but a common Python list, becaue takeSample() is an action.\n",
    "print (dummy_incremented.takeSample(True, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another usefull action is [`count()`](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.count). It returns the number of elements in a RDD. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280\n"
     ]
    }
   ],
   "source": [
    "print (dummy_incremented.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Create a **`dummy_plus_5`** dataset by adding 5 to each element of the **`dummy_rdd`**. You should use the `map()` transformation to achieve the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 8, 10, 12, 14, 16, 18, 20, 22, 24]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1\n",
    "\n",
    "# Add 5 to each element of the dummy_rdd using map() and lambda function.\n",
    "dummy_plus_5 = dummy_rdd.map(lambda x: x + 5)\n",
    "\n",
    "print (dummy_plus_5.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "import sys, os\n",
    "sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "dir_path = os.path.dirname(os.path.realpath('Lab_1/Lab_1_SPARK.ipynb'))\n",
    "sys.path.append('/home/vagrant/.local/lib/python2.7/site-packages')\n",
    "\n",
    "from test_helper_spark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "lab1_test_ex_1(dummy_plus_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "Let's create a new RDD containing values from the **`dummy_incremented`** dataset that are less than 100.\n",
    "\n",
    "Here we use the [`filter()`](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.filter) function. This function is a transformation operation that creates a new RDD from the input RDD by applying the predicate function to each element in it and passing only those elements for which the predicate function returns `True`. Elements that do not return `True` will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98]\n"
     ]
    }
   ],
   "source": [
    "# A predicate function that checks if value is less than 100.\n",
    "# It will be applied to each element of the dataset.\n",
    "def is_less_than_100(value):\n",
    "    return value < 100\n",
    "    \n",
    "dummy_lt_100 = dummy_incremented.filter(is_less_than_100)\n",
    "\n",
    "# Since filter is a transformation, we have to use the collect action\n",
    "# to preform calculations and get the resulting data.\n",
    "print (dummy_lt_100.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "1. Implement an **`is_divisible_by_3()`** function that checks if the number is divisible by 3.\n",
    "2. Create a **`dummy_divisible_by_3`** dataset containing values from the **`dummy_plus_5`** dataset that are divisible by 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 12, 18, 24, 30, 36, 42, 48, 54, 60]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "\n",
    "# Implement a function that checks if value is divisible by 3\n",
    "def is_divisible_by_3(value):\n",
    "    return value % 3 == 0\n",
    "\n",
    "# RDD with elements divisible by 3\n",
    "dummy_divisible_by_3 = dummy_plus_5.filter(is_divisible_by_3)\n",
    "\n",
    "print (dummy_divisible_by_3.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "lab1_test_ex_2(dummy_divisible_by_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More actions\n",
    "\n",
    "There are few other frequently used actions: [first()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.first), [take()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.take), [top()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.top), [takeOrdered()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.takeOrdered), and [reduce()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.reduce).\n",
    " \n",
    "In order to get rough understanding of the data through visual inspection, `first()`, `take()`, `top()`, and `takeOrdered()` actions are used. Note that for the `first()` and `take()` actions, the elements that are returned depend on how the RDD is *partitioned*.\n",
    "\n",
    "* The `take(n)` action returns the first n elements of the RDD. \n",
    "* The `first()` action returns the first element of an RDD, and is equivalent to `take(1)`.\n",
    "* The `takeOrdered(n)` action returns the first n elements of an RDD, using either their natural order or a custom comparator. \n",
    "* The `top(n)` action is similar to `takeOrdered(n)` except that it returns the list in *descending order*.\n",
    "* The `reduce()` action reduces the elements of a RDD to a single value by applying a function that takes two parameters and returns a single value. The function should be commutative and associative, as `reduce()` is applied at the partition level and then again to aggregate results from partitions. If these rules don't hold, the results from `reduce()` will be inconsistent.  Reducing locally at partitions makes `reduce()` very efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Get the first element\n",
    "print (dummy_rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n"
     ]
    }
   ],
   "source": [
    "# Get the first 10 elements\n",
    "print (dummy_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the three smallest elements\n",
    "print (dummy_rdd.takeOrdered(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4559, 4557, 4555, 4553, 4551]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the five largest elements\n",
    "print (dummy_rdd.top(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4559, 4557, 4555, 4553]\n"
     ]
    }
   ],
   "source": [
    "# Pass a lambda function to takeOrdered to reverse the order\n",
    "print (dummy_rdd.takeOrdered(4, lambda s: -s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5198400\n",
      "5198400\n"
     ]
    }
   ],
   "source": [
    "# Getting Python's native add() function\n",
    "from operator import add\n",
    "\n",
    "# Sum the RDD using the reduce() function\n",
    "print (dummy_rdd.reduce(add))\n",
    "\n",
    "# Sum the RDD using reduce() with a lambda function\n",
    "print (dummy_rdd.reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "1. Create a **`dummy_less_than_50`** dataset with numbers less than 50 from the **`dummy_divisible_by_3`**.\n",
    "2. Create a **`dummy_top_5`** dataset with top 5 numbers from the **`dummy_less_than_50`**.\n",
    "3. Create a **`dummy_product`** dataset with a product of all numbers from the **`dummy_less_than_50`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 12, 18, 24, 30, 36, 42, 48]\n",
      "[48, 42, 36, 30, 24]\n",
      "67722117120\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3\n",
    "\n",
    "# RDD with numbers that are less than 50\n",
    "dummy_less_than_50 = dummy_divisible_by_3.filter(lambda x: x < 50)\n",
    "\n",
    "# Get top 5 largest numbers from dummy_less_than_50\n",
    "dummy_top_5 = dummy_less_than_50.top(5)\n",
    "\n",
    "# Multiply all numbers from dummy_less_than_50\n",
    "dummy_product = dummy_less_than_50.reduce(lambda x, y: x * y)\n",
    "\n",
    "print (dummy_less_than_50.collect())\n",
    "print (dummy_top_5)\n",
    "print (dummy_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "lab1_test_ex_3(dummy_less_than_50, dummy_top_5, dummy_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count by value\n",
    "\n",
    "The [`countByValue()`](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.countByValue) action returns the count of each unique value in the RDD as a dictionary that maps values to counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'d': 3, 's': 1, 'r': 3, 'ee': 1, 'rr': 1})\n"
     ]
    }
   ],
   "source": [
    "symbols = sc.parallelize([\"d\", \"s\", \"d\", \"d\", \"r\", \"ee\", \"rr\", \"r\", \"r\"])\n",
    "print (symbols.countByValue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "1. Create a **`letters`** dataset containing all letters and symbols from text except whitespaces.    \n",
    "2. Count the occurrence of each symbol in the **`letters`** RDD (it will be a dictionary) and assign it to a variable **`occurrences`**.\n",
    "\n",
    "*Note: While you can use native Python's `map()` and `replace()` functions to complete the first task, the better approach is to use Spark's `map()` and `filter()` functions, because, unlike the Python's functions, they are designed to work with huge datasets that does not fit in memory of one machine.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'h', 'a', 'v', 'e', 'a', 'c', 'a', 't', '.', 't', 'h', 'e', 'c', 'a', 't', 'i', 's', 'v', 'e', 'r', 'y', 'n', 'i', 'c', 'e', '.', 'i', 'l', 'o', 'v', 'e', 'm', 'y', 'c', 'a', 't', 'v', 'e', 'r', 'y', 'm', 'u', 'c', 'h', '.'] ['i', 'h', 'a', 'v', 'e', 'a', 'c', 'a', 't', '.', 't', 'h', 'e', 'c', 'a', 't', 'i', 's', 'v', 'e', 'r', 'y', 'n', 'i', 'c', 'e', '.', 'i', 'l', 'o', 'v', 'e', 'm', 'y', 'c', 'a', 't', 'v', 'e', 'r', 'y', 'm', 'u', 'c', 'h', '.']\n",
      "['i', 'h', 'a', 'v', 'e', 'a', 'c', 'a', 't', '.', 't', 'h', 'e', 'c', 'a', 't', 'i', 's', 'v', 'e', 'r', 'y', 'n', 'i', 'c', 'e', '.', 'i', 'l', 'o', 'v', 'e', 'm', 'y', 'c', 'a', 't', 'v', 'e', 'r', 'y', 'm', 'u', 'c', 'h', '.'] \n",
      "\n",
      "defaultdict(<class 'int'>, {'i': 4, 'h': 3, 'a': 5, 'v': 4, 'e': 6, 'c': 5, 't': 4, '.': 3, 's': 1, 'r': 2, 'y': 3, 'n': 1, 'l': 1, 'o': 1, 'm': 2, 'u': 1})\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4\n",
    "\n",
    "text = \"I have a cat. The cat is very nice. I love my cat very much.\"\n",
    "\n",
    "to_lower = lambda x: x.lower()\n",
    "\n",
    "# The first approach, using Python's functions\n",
    "python_letters = list(map(to_lower, text.replace(' ', '')))\n",
    "\n",
    "# Create an RDD containing all letters and symbols from the text, except whitespaces.\n",
    "letters = sc.parallelize(text) \\\n",
    "            .filter(lambda x: x != ' ') \\\n",
    "            .map(to_lower)\n",
    "\n",
    "print(python_letters, letters.collect())\n",
    "# They must be equal\n",
    "assert python_letters == letters.collect()\n",
    "        \n",
    "# Count the occurrence of each symbol\n",
    "occurrence = letters.countByValue()\n",
    "\n",
    "print (letters.collect(), '\\n')\n",
    "print (occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "lab1_test_ex_4(letters, occurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat map\n",
    "\n",
    "The [flatMap()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.flatMap) transformation is similar to `map()`, except that with `flatMap()` each input item can be mapped to zero or more output elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From arrays [10, 10, 20, 20, 30, 30]\n",
      "From tuples [10, 10, 20, 20, 30, 30]\n",
      "From sets [10, 20, 30]\n"
     ]
    }
   ],
   "source": [
    "# Using flatMap() you can flatten any collection\n",
    "foo = lambda x: [x * 10, x * 10]\n",
    "bar = lambda x: (x * 10, x * 10)\n",
    "baz = lambda x: {x * 10, x * 10}\n",
    "\n",
    "numbers = sc.parallelize([1, 2, 3])\n",
    "\n",
    "print (\"From arrays\", numbers.flatMap(foo).collect())\n",
    "print (\"From tuples\", numbers.flatMap(bar).collect())\n",
    "print (\"From sets\", numbers.flatMap(baz).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Given a **`sample`** dataset:\n",
    "1. Map each element of the **`sample`** to a pair: (original data, original data + letter 's') and assign it to a variable **`pairs`**.\n",
    "    >For example, `[\"data\", \"science\"]` becomes `[(\"data\", \"datas\"), (\"science\", \"sciences\")]`.\n",
    "2. Using the `flatMap()` directly to the **`sample`**, create a flat RDD of pairs and assign it to a variable **`flat_pairs`**.\n",
    "    >For example, `[\"data\", \"science\"]` becomes `[\"data\", \"datas\", \"science\", \"sciences\"]`.\n",
    "3. Using the `flatMap()` function, flatten the **`pairs`** dataset and assign it to a variable **`flattened_pairs`**.\n",
    "    >For example, `[(\"data\", \"datas\"), (\"science\", \"sciences\")]` becomes `[\"data\", \"datas\", \"science\", \"sciences\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', '1s'), ('2', '2s'), ('3', '3s'), ('4', '4s'), ('5', '5s')]\n",
      "['1', '1s', '2', '2s', '3', '3s', '4', '4s', '5', '5s']\n",
      "['1', '1s', '2', '2s', '3', '3s', '4', '4s', '5', '5s']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5\n",
    "\n",
    "sample = sc.parallelize([str(x) for x in range(1, 40)], 4)\n",
    "\n",
    "# Create an RDD of pairs. E.g. [(1, 2), (3, 4)].\n",
    "pairs = sample.map(lambda x: (x, x + 's'))\n",
    "\n",
    "# Create a flat RDD of pairs by transforming the pairs variable. E.g. [1, 2, 3, 4].\n",
    "flat_pairs = sample.flatMap(lambda x: (x, x + 's'))\n",
    "\n",
    "# Create a flat RDD of pairs by flattening the flat_pairs variable. E.g. [1, 2, 3, 4].\n",
    "flattened_pairs = pairs.flatMap(lambda x: x)\n",
    "\n",
    "print (pairs.take(5))\n",
    "print (flat_pairs.take(10))\n",
    "print (flattened_pairs.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "lab1_test_ex_5(sample, flat_pairs, flattened_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key-value pairs\n",
    "\n",
    "An approach using the `map()` transformation is often used to create key-value pairs.\n",
    "\n",
    "Let's examine transformations that are used with key-value pairs: [groupByKey()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.groupByKey) and [reduceByKey()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.reduceByKey).         \n",
    "\n",
    "The `groupByKey()` transformation groups the values for each key in the RDD into a single sequence.\n",
    "\n",
    "The `reduceByKey()` transformation gathers together pairs that have the same key and applies a reducer function to two associated values at a time. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions.\n",
    "\n",
    "While both the `groupByKey()` and `reduceByKey()` transformations can often be used to solve the same problem and will produce the same answer, the `reduceByKey()` transformation works much better for large distributed datasets. \n",
    "\n",
    "Here are more transformations to prefer over `groupByKey()`:\n",
    "  * [combineByKey()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.combineByKey) \n",
    "can be used when you are combining elements but your return type differs from your input value type.\n",
    "  * [foldByKey()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.foldByKey) \n",
    "merges the values for each key using an associative function and a neutral \"zero value\".      \n",
    "\n",
    "Now let's go through a simple `groupByKey()` and `reduceByKey()` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mango', 2), ('banana', 1), ('apple', 2), ('papaya', 1), ('prune', 1)]\n",
      "[('mango', 2), ('banana', 1), ('apple', 2), ('papaya', 1), ('prune', 1)]\n",
      "defaultdict(<class 'int'>, {'apple': 2, 'banana': 1, 'papaya': 1, 'mango': 2, 'prune': 1})\n"
     ]
    }
   ],
   "source": [
    "fruits = sc.parallelize([\"apple\", \"banana\", \"apple\", \"papaya\", \"mango\", \"prune\", \"mango\"], 4)\n",
    "\n",
    "fruit_pairs = fruits.map(lambda x: (x, 1))\n",
    "\n",
    "# 3 different ways to sum by key:\n",
    "\n",
    "# The dumb way for those, who doesn't know about mapValues() function.\n",
    "# print (fruit_pairs.groupByKey().map(lambda (k, v): (k, sum(v))).collect())\n",
    "\n",
    "# Using mapValues(), which is recommended when they key doesn't change.\n",
    "print (fruit_pairs.groupByKey().mapValues(lambda x: sum(x)).collect())\n",
    "\n",
    "# reduceByKey() is more efficient / scalable.\n",
    "print (fruit_pairs.reduceByKey(add).collect())\n",
    "\n",
    "# countByKey() is an action that returns a dict with the number of elements for each key\n",
    "print (fruit_pairs.countByKey())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`distinct()`](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.distinct) transformation is used to get a list of unique elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'apple', 'papaya', 'mango', 'prune', 'mango']\n",
      "['mango', 'banana', 'apple', 'papaya', 'prune']\n"
     ]
    }
   ],
   "source": [
    "print (fruits.collect())\n",
    "print (fruits.distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "1. Create a **`numbers`** dataset from the existing collection: `[1, 3, 4, 1, 4, 7, 12, 3, 4, 2, 2, 6, 2, 1, 9]`.\n",
    "2. Create a **`number_pairs`** dataset of paris, in which a key is an element from the **`numbers`** RDD and a value is `1`.\n",
    "3. Create a **`number_duplicates`** dataset of pairs, in which keys are unique and values contain the number of duplicates of the corresponding key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (3, 1), (4, 1), (1, 1), (4, 1), (7, 1), (12, 1), (3, 1), (4, 1), (2, 1), (2, 1), (6, 1), (2, 1), (1, 1), (9, 1)]\n",
      "[(1, 3), (3, 2), (4, 3), (7, 1), (12, 1), (2, 3), (6, 1), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6\n",
    "\n",
    "numbers_raw = [1, 3, 4, 1, 4, 7, 12, 3, 4, 2, 2, 6, 2, 1, 9]\n",
    "\n",
    "# Create an RDD with numbers\n",
    "numbers = sc.parallelize(numbers_raw)\n",
    "\n",
    "# Create an RDD of key-value pairs,\n",
    "# where first element is original data from numbers and second element is 1\n",
    "number_pairs = numbers.map(lambda x: (x, 1))\n",
    "\n",
    "# Count the number of duplicates\n",
    "number_duplicates = number_pairs.reduceByKey(add)\n",
    "\n",
    "print (number_pairs.collect())\n",
    "print (number_duplicates.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "lab1_test_ex_6(number_pairs, number_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching RDDs\n",
    "\n",
    "If you need to use the same RDD more than once, it can be usefull to cache it using the [`cache()`](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD.cache) function. However, if you cache too many RDDs and Spark runs out of memory, it will delete the least recently used RDD first. The RDD will be automatically recreated when accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Name the RDD.\n",
    "numbers.setName('My Filtered RDD')\n",
    "\n",
    "# Cache the RDD.\n",
    "numbers.cache()\n",
    "\n",
    "# Is it cached?\n",
    "print (numbers.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple inventory management system modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the lists of inventories of the 2 warehouses.\n",
    "inventory_raw_1 = ['Hammer', 'nail', 'Nail', 'screwdriver', 'Backpack',\n",
    "                   'Bolt D9', 'Nut D9', 'Bolt D9', 'Nut D9', 'Bolt D9', 'nut D12',\n",
    "                   'Bolt D12', 'nut D9', 'Bolt D9', 'Nut D12']\n",
    "inventory_raw_2 = ['Bolt D8', 'nut D8','Screwdriver', 'Backpack',\n",
    "                   'Bolt D9', 'screwdriver', 'backpack', 'Bolt D9', 'First Aid Kit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create an RDD for each warehouse. Be aware that some goods are written with uppercase letters. You have to use `lower()` function from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hammer', 'nail', 'nail', 'screwdriver', 'backpack', 'bolt d9', 'nut d9', 'bolt d9', 'nut d9', 'bolt d9', 'nut d12', 'bolt d12', 'nut d9', 'bolt d9', 'nut d12']\n",
      "['bolt d8', 'nut d8', 'screwdriver', 'backpack', 'bolt d9', 'screwdriver', 'backpack', 'bolt d9', 'first aid kit']\n"
     ]
    }
   ],
   "source": [
    "# inventory_2 contains the inventory of the first warehouse.\n",
    "inventory_1 = sc.parallelize(inventory_raw_1, 4).map(lambda x: x.lower())\n",
    "\n",
    "# inventory_2 contains the inventory of the second warehouse.\n",
    "inventory_2 = sc.parallelize(inventory_raw_2, 4).map(lambda x: x.lower())\n",
    "\n",
    "print (inventory_1.collect())\n",
    "print (inventory_2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [union()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html?highlight=union#pyspark.RDD.union) function to combine two datasets into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnionRDD[72] at union at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "# Combine two inventories into one.\n",
    "inventory = inventory_1.union(inventory_2)\n",
    "print (inventory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate amount of goods by using `map()` and `reduceByKey()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nail', 2), ('bolt d12', 1), ('nut d8', 1), ('backpack', 3), ('nut d9', 3), ('screwdriver', 3), ('bolt d9', 6), ('nut d12', 2), ('bolt d8', 1), ('first aid kit', 1), ('hammer', 1)]\n"
     ]
    }
   ],
   "source": [
    "inventory_pairs = inventory.map(lambda x: (x, 1)).reduceByKey(add)\n",
    "print (inventory_pairs.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "1. Find out which goods are stored in warehouse 1 but not in warehouse 2 and save the result as an **`inventory_diff`** variable. You can use the [subtract()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html?highlight=subtract#pyspark.RDD.subtract) function.\n",
    "2. Create an **`inventory_diff_sorted`** dataset, in which elements of the **`inventory_diff`** are sorted by key. Use the [sortByKey()](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html?highlight=sortbykey#pyspark.RDD.sortByKey) function to complete the task.\n",
    "\n",
    "*Note: `sortByKey()` accepts a boolean parameter, which specifies a sorting order (default is ascending).* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nail', 2), ('bolt d12', 1), ('nut d9', 3), ('nut d12', 2), ('hammer', 1)]\n",
      "[('bolt d12', 1), ('hammer', 1), ('nail', 2), ('nut d12', 2), ('nut d9', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 7\n",
    "\n",
    "# Subtract inventory_2 from inventory_1.\n",
    "# Create an RDD of pairs (goods, number of goods).\n",
    "# You may want to use map and reduceByKey.\n",
    "inventory_diff = inventory_1.subtract(inventory_2) \\\n",
    "                            .map(lambda x: (x, 1)) \\\n",
    "                            .reduceByKey(add)\n",
    "\n",
    "# Sort items alphabetically\n",
    "inventory_diff_sorted = inventory_diff.sortByKey()\n",
    "\n",
    "print (inventory_diff.collect())\n",
    "print (inventory_diff_sorted.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "lab1_test_ex_7(inventory_diff, inventory_diff_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congradulations!** Now you have a basic understanding of Apache Spark's transformations and actions.\n",
    "\n",
    "Remember that Spark's API is really huge, so if you want to know more about it's capabilities, check the [official documentation](http://spark.apache.org/docs/1.6.3/api/python/pyspark.html).\n",
    "\n",
    "In the next practical lesson we will examine the `pyspark.sql` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Presented by <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"http://datascience-school.com\">datascience-school.com</a></h3></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
