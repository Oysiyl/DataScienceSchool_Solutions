{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Lab 2. Text preprocessing</h1>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. REGULAR EXPRESSIONS \n",
    "\n",
    "### re Python library<br>\n",
    "Using the methods find and replace, we can find and fix any portion of the text, but there is one drawback - we need to know exactly what we're looking for. But if we need to find a phone number, we do not know, but we know the format. The human brain just to find the right, but to find is not capable. To help us with this problem was created module `re`.<br>\n",
    "\n",
    "Regular expressions (called REs, or regexes, or regex patterns) are essentially a tiny, highly specialized programming language embedded inside Python and made available through the re module. Using this little language, you specify the rules for the set of possible strings that you want to match; this set might contain English sentences, or e-mail addresses, or TeX commands, or anything you like. You can then ask questions such as “Does this string match the pattern?”, or “Is there a match for the pattern anywhere in this string?”. \n",
    "\n",
    "<i>Regular expression syntax</i>\n",
    "<table> \n",
    "  <tbody>\n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>Metacharacter(s)</b></p>\n",
    "       </td> <td> \n",
    "        <p align=\"center\"><b>Description</b></p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>.</b></p>\n",
    "       </td> <td> \n",
    "        <p>Normally matches any character except a newline. Within square brackets the dot is literal.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>( )</b></p>\n",
    "       </td> <td> \n",
    "        <p>Groups a series of pattern elements to a single element. When you match a pattern within parentheses, you can use any of $1, $2, ... later to refer to the previously matched pattern.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>+</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches the preceding pattern element one or more times.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>?</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches the preceding pattern element zero or one times.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>`*`</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches the preceding pattern element zero or more times.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>{M,N}</b></p>\n",
    "       </td> <td> \n",
    "        <p>Denotes the minimum M and the maximum N match count.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>[...]</b></p>\n",
    "       </td> <td> \n",
    "        <p>Denotes a set of possible character matches.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>|</b></p>\n",
    "       </td> <td> \n",
    "        <p>Separates alternate possibilities.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>\\b</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches a word boundary.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>\\w</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches an alphanumeric character, including whitespace.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>\\W</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches a non-alphanumeric character, excluding whitespace.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>\\s</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches a whitespace character (space, tab, newline, form feed)</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>\\S</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches anything BUT a whitespace.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>\\d</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches a digit, same as [0-9].</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>\\D</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches a non-digit.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>^</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches the beginning of a line or string.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>$</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches the end of a line or string.</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>\\A</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches the beginning of a string (but not an internal line).</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>\\Z</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches the end of a string (but not an internal line).</p>\n",
    "       </td> </tr>\n",
    "   \n",
    "    <tr> <td> \n",
    "        <p align=\"center\"><b>[^...]</b></p>\n",
    "       </td> <td> \n",
    "        <p>Matches every character except the ones inside brackets.</p>\n",
    "       </td> </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "You can also use REs to modify a string or to split it apart in various ways.\n",
    "Here are the most commonly used ones:\n",
    "\n",
    "- `re.match(pattern, string)` - search for the specified pattern in the beginning of the string;\n",
    "- `re.search(pattern, string)` - similar to `match()`, but it search not only at the beginning of the string; `group()` method allows to get the result of searching;\n",
    "- `re.findall(pattern, repl, string)` - method returns a list of all matches found;\n",
    "- `re.split(pattern, string, [maxsplit=0])` - divides the string in a predetermined pattern;\n",
    "- `re.sub(pattern, repl, string)` - looking for a pattern in a string and replaces it with the specified string;\n",
    "- `re.compile(pattern, repl, string)` - collects a regular expression into a separate entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All patern and function you can see run code `re?`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a few examples to consolidate knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import module\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = 'This is a test string for regular expression: 16-05-2016 10:20!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16', '05', '2016', '10', '20']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all numbers\n",
    "re.findall('\\d+', s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first number with at least 4 digits\n",
    "re.search('\\d{4,}', s).group() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', 'string', 'for', 'regular', 'expression', '16', '05', '2016', '10', '20']\n"
     ]
    }
   ],
   "source": [
    "# Find only letters or digits\n",
    "print (re.findall('\\w+', s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ': ', '-', '-', ' ', ':', '!']\n"
     ]
    }
   ],
   "source": [
    "# Find all non-letters or non-digits\n",
    "print (re.findall('[\\W]+', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10:20'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display time\n",
    "re.search('\\d+:(\\d+)', s).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':', ':']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all colons\n",
    "re.findall('[:]+', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['his', 'is', 'a', 'test', 'string', 'for', 'regular', 'expression']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all small letter\n",
    "re.findall('[a-z]+', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', 'string', 'for', 'regular', 'expression']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display only letters\n",
    "re.findall('[A-Za-z]+', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get words from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['re', 'split', 'pattern', 'string', 'maxsplit', 'divides', 'the', 'string', 'in', 'a', 'predetermined', 'pattern']\n"
     ]
    }
   ],
   "source": [
    "text = 're.split(pattern, string, [maxsplit=0]) - divides the string in a predetermined pattern'\n",
    "TEXT_SPLIT = re.compile(r\"[\\W\\d]+\")\n",
    "result = re.split(TEXT_SPLIT, text)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check email is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'test@mail.ru'\n",
    "bool(re.match(r'^([a-z0-9_\\.-]+)@([a-z0-9_\\.-]+)\\.([a-z\\.]{2,6})$',s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test@mail.ru\n"
     ]
    }
   ],
   "source": [
    "EMAIL_REGEX = re.compile(r\"[\\w.]+@[\\w.]+\")\n",
    "s = 'My email is test@mail.ru'\n",
    "\n",
    "result = re.search(EMAIL_REGEX,s)\n",
    "print (result.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phones search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+380 (55) 12-12-123', '+380 (55) 12-12-124']\n"
     ]
    }
   ],
   "source": [
    "PHONE_REGEX = re.compile(r\".[\\d]+ \\([\\d]+\\) [\\d]+.[\\d]+.[\\d]+\")\n",
    "s = 'My phone is +380 (55) 12-12-123, my friend\\'s phone is +380 (55) 12-12-124'\n",
    "\n",
    "result = re.findall(PHONE_REGEX,s)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hide phone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My phone is +380 (55) XX-XX-XXX, my friend's phone is +380 (55) XX-XX-XXX\n"
     ]
    }
   ],
   "source": [
    "PHONE_HIDE_REGEX = re.compile(r\"[\\d]+.[\\d]+.[\\d]+\")\n",
    "result = re.sub(PHONE_HIDE_REGEX, 'XX-XX-XXX', s)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have saved a Wikipedia page about regular expressions. Let us consider use of regular expressions to parse web pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\" class=\"client-nojs\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>Regular expression - Wikipedia, the free encyclopedia</title>\\n<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)client-nojs(\\\\s|$)/, \"$1client-js$2\" );</'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_expressions_page = open('regular_expression.html').read()\n",
    "regular_expressions_page[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Html code looks nice in the browser, but in a text written in html code creates a lot of waste.<br>\n",
    "Select the title of the page and main text.\n",
    "Main Title always is placed on a tag `<h1>`. The main text of the wikipedia page is stored in the tag `<p>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Regular expression']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile('<h1.+\\>(.+)\\<.+h1\\>')\n",
    "title = re.findall(pattern,regular_expressions_page)\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, a more complex pattern, we can select all the main headlines to the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Patterns',\n",
       " 'History',\n",
       " 'Basic concepts',\n",
       " 'Formal language theory',\n",
       " 'Formal definition',\n",
       " 'Expressive power and compactness',\n",
       " 'Deciding equivalence of regular expressions',\n",
       " 'Syntax',\n",
       " 'Delimiters',\n",
       " 'Standards',\n",
       " 'POSIX basic and extended',\n",
       " 'POSIX extended',\n",
       " 'Character classes',\n",
       " 'Perl',\n",
       " 'Lazy matching',\n",
       " 'Fuzzy regexps',\n",
       " 'Implementations and running times',\n",
       " 'Unicode',\n",
       " 'Uses',\n",
       " 'Examples',\n",
       " 'Induction',\n",
       " 'See also',\n",
       " 'Notes',\n",
       " 'References',\n",
       " 'External links']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile('\\<h[0-9]*\\>\\<span.*?\\>([A-Za-z ]+)\\</span\\>.*\\</h[0-9]*\\>')\n",
    "headings = re.findall(pattern,regular_expressions_page)\n",
    "headings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\\<h[0-9]*\\>(.+)\\</h[0-9]*\\>`- It does not work, because Wikipedia text in headings wraps in the span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the first 4 paragraphs of text (introductory part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In <a href=\"/wiki/Theoretical_computer_science\" title=\"Theoretical computer science\">theoretical computer science</a> and <a href=\"/wiki/Formal_language_theory\" title=\"Formal language theory\" class=\"mw-redirect\">formal language theory</a>, a <b>regular expression</b> (sometimes called a <b>rational expression</b>)<sup id=\"cite_ref-Mitkov2003_1-0\" class=\"reference\"><a href=\"#cite_note-Mitkov2003-1\">[1]</a></sup><sup id=\"cite_ref-Lawson2003_2-0\" class=\"reference\"><a href=\"#cite_note-Lawson2003-2\">[2]</a></sup> is a sequence of <a href=\"/wiki/Character_(computing)\" title=\"Character (computing)\">characters</a> that define a search pattern, mainly for use in <a href=\"/wiki/Pattern_matching\" title=\"Pattern matching\">pattern matching</a> with <a href=\"/wiki/String_(computer_science)\" title=\"String (computer science)\">strings</a>, or <a href=\"/wiki/String_matching\" title=\"String matching\" class=\"mw-redirect\">string matching</a>, i.e. \"find and replace\"-like operations. The concept arose in the 1950s, when the American mathematician <a href=\"/wiki/Stephen_Cole_Kleene\" title=\"Stephen Cole Kleene\">Stephen Kleene</a> formalized the description of a <i><a href=\"/wiki/Regular_language\" title=\"Regular language\">regular language</a></i>, and came into common use with the <a href=\"/wiki/Unix\" title=\"Unix\">Unix</a> text processing utilities <a href=\"/wiki/Ed_(text_editor)\" title=\"Ed (text editor)\">ed</a>, an editor, and <a href=\"/wiki/Grep\" title=\"Grep\">grep</a>, a <a href=\"/wiki/Filter_(computer_science)\" title=\"Filter (computer science)\" class=\"mw-redirect\">filter</a>. In modern usage, \"regular expressions\" are often distinguished from the derived, but fundamentally <i>distinct</i> concepts of <b>regex</b> or <b>regexp</b>, which no longer describe a regular language. See <a href=\"#Patterns_for_non-regular_languages\">below</a> for details. Regexps are so useful in computing that the various systems to specify regexps have evolved to provide both a <i>basic</i> and <i>extended</i> standard for the grammar and syntax; <i>modern</i> regexps heavily augment the standard. Regexp processors are found in several <a href=\"/wiki/Search_engine\" title=\"Search engine\" class=\"mw-redirect\">search engines</a>, search and replace dialogs of several <a href=\"/wiki/Word_processor\" title=\"Word processor\">word processors</a> and <a href=\"/wiki/Text_editor\" title=\"Text editor\">text editors</a>, and in the command lines of <a href=\"/wiki/Category:Unix_text_processing_utilities\" title=\"Category:Unix text processing utilities\">text processing utilities</a>, such as <a href=\"/wiki/Sed\" title=\"Sed\">sed</a> and <a href=\"/wiki/AWK\" title=\"AWK\">AWK</a>. Many <a href=\"/wiki/Programming_language\" title=\"Programming language\">programming languages</a> provide regexp capabilities, some built-in, for example <a href=\"/wiki/Perl_language_structure#Regular_expressions\" title=\"Perl language structure\">Perl</a>, <a href=\"/wiki/JavaScript\" title=\"JavaScript\">JavaScript</a>, <a href=\"/wiki/Ruby_(programming_language)\" title=\"Ruby (programming language)\">Ruby</a>, <a href=\"/wiki/AWK\" title=\"AWK\">AWK</a>, and <a href=\"/wiki/Tcl\" title=\"Tcl\">Tcl</a>, and others via a <a href=\"/wiki/Standard_library\" title=\"Standard library\">standard library</a>, for example <a href=\"/wiki/.NET_Framework\" title=\".NET Framework\">.NET languages</a>, <a href=\"/wiki/Java_(programming_language)\" title=\"Java (programming language)\">Java</a>, <a href=\"/wiki/Python_(programming_language)\" title=\"Python (programming language)\">Python</a>, <a href=\"/wiki/C_POSIX_library#regex.h\" title=\"C POSIX library\">POSIX C</a>, and <a href=\"/wiki/C%2B%2B\" title=\"C++\">C++</a> (since <a href=\"/wiki/C%2B%2B11#regex\" title=\"C++11\">C++11</a>). Most other languages offer regexps via a library.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile('\\<p\\>(.+)\\</p\\>')\n",
    "text = re.findall(pattern,regular_expressions_page)\n",
    "intro = ' '.join(text[:4])\n",
    "intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information already more readable, but still we are hindered markup tags. Let's remove them completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In theoretical computer science and formal language theory, a regular expression (sometimes called a rational expression)[1][2] is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching, i.e. \"find and replace\"-like operations. The concept arose in the 1950s, when the American mathematician Stephen Kleene formalized the description of a regular language, and came into common use with the Unix text processing utilities ed, an editor, and grep, a filter. In modern usage, \"regular expressions\" are often distinguished from the derived, but fundamentally distinct concepts of regex or regexp, which no longer describe a regular language. See below for details. Regexps are so useful in computing that the various systems to specify regexps have evolved to provide both a basic and extended standard for the grammar and syntax; modern regexps heavily augment the standard. Regexp processors are found in several search engines, search and replace dialogs of several word processors and text editors, and in the command lines of text processing utilities, such as sed and AWK. Many programming languages provide regexp capabilities, some built-in, for example Perl, JavaScript, Ruby, AWK, and Tcl, and others via a standard library, for example .NET languages, Java, Python, POSIX C, and C++ (since C++11). Most other languages offer regexps via a library.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro = re.sub(\"\\<.+?\\>\", lambda x:'', intro)\n",
    "intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also delete footnote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In theoretical computer science and formal language theory, a regular expression (sometimes called a rational expression) is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching, i.e. \"find and replace\"-like operations. The concept arose in the 1950s, when the American mathematician Stephen Kleene formalized the description of a regular language, and came into common use with the Unix text processing utilities ed, an editor, and grep, a filter. In modern usage, \"regular expressions\" are often distinguished from the derived, but fundamentally distinct concepts of regex or regexp, which no longer describe a regular language. See below for details. Regexps are so useful in computing that the various systems to specify regexps have evolved to provide both a basic and extended standard for the grammar and syntax; modern regexps heavily augment the standard. Regexp processors are found in several search engines, search and replace dialogs of several word processors and text editors, and in the command lines of text processing utilities, such as sed and AWK. Many programming languages provide regexp capabilities, some built-in, for example Perl, JavaScript, Ruby, AWK, and Tcl, and others via a standard library, for example .NET languages, Java, Python, POSIX C, and C++ (since C++11). Most other languages offer regexps via a library.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro = re.sub(\"\\[.+?\\]\", lambda x:'', intro)\n",
    "intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see after the modifier amount, we used a `?`. It makes our non-greedy quantifier.\n",
    "If we did not do it then the search would continue until he met the last bow, because all shackle also fit the template `.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we looked at how to search a phone number, let's consider a more complex example.<br>\n",
    "\n",
    "Here are the combinations of possible phone numbers that we have to parse. We should be able to get the area code, the trunk, and the rest of the phone number 5309.\n",
    "- 234-123-4567\n",
    "- 234 123 4567\n",
    "- 234.123.4567\n",
    "- (234) 123-4567\n",
    "- 23-234-123-4567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('342', '223', '4234'), ('233', '234', '2345')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_pattern = re.compile(r'(\\d{3})\\D*(\\d{3})\\D*(\\d{4})')\n",
    "phone_pattern.findall('My phone is 3422234234, my friend\\'s phone is +1(233)234-2345')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python allows us to do this with something called verbose regular expressions. A verbose regular expression is different from a compact regular expression in two ways:\n",
    "\n",
    "- Whitespace is ignored. Spaces, tabs, and carriage returns are not matched as spaces, tabs, and carriage returns. They're not matched at all. If we want to match a space in a verbose regular expression, we'll need to escape it by putting a backslash in front of it)\n",
    "- Comments are ignored. A comment in a verbose regular expression is just like a comment in Python code: it starts with a # character and goes until the end of the line. In this case it's a comment within a multi-line string instead of within our source code, but it works the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('234', '567', '8912')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_pattern = re.compile(r'''\n",
    "                # don't match beginning of string, number can start anywhere\n",
    "    (\\d{3})     # area code is 3 digits (e.g. '415')\n",
    "    \\D*         # optional separator is any number of non-digits\n",
    "    (\\d{3})     # trunk is 3 digits (e.g. '867')\n",
    "    \\D*         # optional separator\n",
    "    (\\d{4})     # rest of number is 4 digits (e.g. '5309')\n",
    "    ''', re.VERBOSE)\n",
    "phone_pattern.search('phone is 1-(234) 567.8912 bad').groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing to remember when using verbose regular expressions is that we need to pass an extra argument when working with them: <b>re.VERBOSE</b> is a constant defined in the re module that signals that the pattern should be treated as a verbose regular expression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Conditional Regular Expression\n",
    "A special construct `(?ifthen|else)` allows you to create conditional regular expressions. If the if part evaluates to true, then the regex engine will attempt to match the then part. Otherwise, the else part is attempted instead.\n",
    "\n",
    "For example, we need to select all words that contain a `reg`, and here we need condition, because the position of `reg` not known in advance. Transforming the list set, we will remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fperlreguts',\n",
       " 'Fregex',\n",
       " 'Fregexp',\n",
       " 'erregular',\n",
       " 'non-regular',\n",
       " 'perlreguts',\n",
       " 'regex',\n",
       " 'regex-based',\n",
       " 'regexen',\n",
       " 'regexes',\n",
       " 'regexp',\n",
       " 'regexps',\n",
       " 'registered',\n",
       " 'regolare',\n",
       " 'regular',\n",
       " 'regular-expression-syntax',\n",
       " 'regularne',\n",
       " 'regulat',\n",
       " 'rregullt'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r\"([A-Za-z\\-]*(?=reg)[A-Za-z\\-]*)\")\n",
    "reg = set(re.findall(pattern,regular_expressions_page))\n",
    "reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If something is incomprehensible, you can always see the official documentation for <a href=\"https://docs.python.org/2/howto/regex.html\">Regular Expression</a>.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Exercise  1\n",
    "As you saw, Python module `re` serves among other things for extraction of the necessary data from some text. \n",
    "\n",
    ">The following code returns the content of [this](https://en.wikipedia.org/wiki/London) page about London. Here we use the [`requests`](http://docs.python-requests.org/en/master/) Python library to get Web page data by its URL:\n",
    ">\n",
    "    import requests\n",
    "    r = requests.get('https://en.wikipedia.org/wiki/London')\n",
    "    text = r.text.encode('utf-8')\n",
    "\n",
    "> With the help of `re` Python library make the following tasks with `text` variable:\n",
    "> 1. Find all unique digit numbers containg the digit 9 (in any place) and after the small letter. Write results to the `digits9` variable as a Python list.\n",
    "> 2. Extract all URLs containing \"`https://`\" and not containing `%`. Write results to the `https` list variable.\n",
    "> 3. Write all unique names of classes and ids in `<div>` HTML tag into `classes` and `ids` list variables. *Hint*: you need searching  blocks like this `<div id=\"mw-page-base\" class=\"noprint\"></div>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.get('https://en.wikipedia.org/wiki/London')\n",
    "text = r.text.encode('utf-8')\n",
    "print(text[:100])\n",
    "text = r.text\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e379434', 'y93', 'a3870951', 'q9', 'p9', 'n1981', 'm2009', 'v9', 'i1993', 'a9', 'o19', 'f9', 'd19', 'e1980', 'z9', 'n9567', 's1998', 'a1988', 'w1991', 'd2009', 'o90402', 'n35311229', 'd9', 'l09', 'b19', 'b968', 'n91', 's1997', 'e129867', 'e9', 'd09', 'a8194911', 'r889293853', 'n79005665', 'n1993', 's2009', 'k1994', 'g1911', 'b119861367', 'g1991', 'e2496158', 'y0996', 'd1999']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('[a-z][0-9]*(?=9)[0-9]*')\n",
    "digits9 = list(set(re.findall(pattern, text)))\n",
    "\n",
    "print(digits9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://maps', 'https://upload', 'https://en', 'https://maps', 'https://london', 'https://en', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://hdi', 'https://www', 'https://web', 'https://www', 'https://web', 'https://web', 'https://www', 'https://web', 'https://www', 'https://www', 'https://newsroom', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://money', 'https://www', 'https://foreignpolicy', 'https://web', 'https://www', 'https://web', 'https://web', 'https://web', 'https://www', 'https://web', 'https://www', 'https://web', 'https://www', 'https://web', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://doi', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://books', 'https://web', 'https://web', 'https://web', 'https://www', 'https://www', 'https://web', 'https://web', 'https://books', 'https://www', 'https://www', 'https://archive', 'https://en', 'https://doi', 'https://web', 'https://web', 'https://books', 'https://web', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://books', 'https://www', 'https://books', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://londondatastore', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://web', 'https://www', 'https://data', 'https://www', 'https://web', 'https://web', 'https://www', 'https://web', 'https://web', 'https://web', 'https://www', 'https://books', 'https://www', 'https://web', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://web', 'https://books', 'https://web', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://londonist', 'https://londonist', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://eip', 'https://eip', 'https://www', 'https://www', 'https://web', 'https://www', 'https://web', 'https://books', 'https://web', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://web', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://web', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://pqasb', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://web', 'https://www', 'https://atkearney', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://web', 'https://newsroom', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://careers', 'https://www', 'https://www', 'https://corporate', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://web', 'https://web', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://books', 'https://web', 'https://www', 'https://web', 'https://www', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://web', 'https://www', 'https://web', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://web', 'https://web', 'https://www', 'https://web', 'https://web', 'https://www', 'https://www', 'https://www', 'https://www', 'https://web', 'https://www', 'https://www', 'https://web', 'https://www', 'https://commons', 'https://commons', 'https://en', 'https://en', 'https://en', 'https://en', 'https://en', 'https://en', 'https://en', 'https://www', 'https://www', 'https://catalogue', 'https://d', 'https://id', 'https://musicbrainz', 'https://catalog', 'https://id', 'https://aleph', 'https://nla', 'https://www', 'https://viaf', 'https://www', 'https://en', 'https://en', 'https://donate', 'https://www', 'https://commons', 'https://en', 'https://en', 'https://en', 'https://en', 'https://ace', 'https://kbd', 'https://ady', 'https://af', 'https://als', 'https://am', 'https://ang', 'https://ab', 'https://ar', 'https://an', 'https://arc', 'https://roa', 'https://frp', 'https://ast', 'https://gn', 'https://av', 'https://ay', 'https://az', 'https://azb', 'https://bm', 'https://bn', 'https://zh', 'https://map', 'https://ba', 'https://be', 'https://be', 'https://bh', 'https://bcl', 'https://bi', 'https://bg', 'https://bar', 'https://bo', 'https://bs', 'https://br', 'https://bxr', 'https://ca', 'https://cv', 'https://ceb', 'https://cs', 'https://cbk', 'https://ny', 'https://sn', 'https://co', 'https://cy', 'https://da', 'https://se', 'https://pdc', 'https://de', 'https://dsb', 'https://dty', 'https://et', 'https://el', 'https://eml', 'https://myv', 'https://es', 'https://eo', 'https://ext', 'https://eu', 'https://ee', 'https://fa', 'https://hif', 'https://fo', 'https://fr', 'https://fy', 'https://ff', 'https://ga', 'https://gv', 'https://gag', 'https://gd', 'https://gl', 'https://inh', 'https://gan', 'https://gu', 'https://gom', 'https://hak', 'https://ko', 'https://ha', 'https://haw', 'https://hy', 'https://hi', 'https://hsb', 'https://hr', 'https://io', 'https://ig', 'https://ilo', 'https://id', 'https://ia', 'https://ie', 'https://os', 'https://zu', 'https://is', 'https://it', 'https://he', 'https://jv', 'https://kbp', 'https://kl', 'https://kn', 'https://krc', 'https://ka', 'https://csb', 'https://kk', 'https://kw', 'https://sw', 'https://kv', 'https://ht', 'https://ku', 'https://ky', 'https://mrj', 'https://lad', 'https://lbe', 'https://lo', 'https://lrc', 'https://la', 'https://lv', 'https://lb', 'https://lez', 'https://lt', 'https://lij', 'https://li', 'https://ln', 'https://lfn', 'https://olo', 'https://jbo', 'https://lmo', 'https://hu', 'https://mai', 'https://mk', 'https://mg', 'https://ml', 'https://mt', 'https://mi', 'https://mr', 'https://xmf', 'https://arz', 'https://mzn', 'https://ms', 'https://cdo', 'https://mwl', 'https://mn', 'https://my', 'https://na', 'https://fj', 'https://nl', 'https://nds', 'https://ne', 'https://new', 'https://ja', 'https://nap', 'https://ce', 'https://frr', 'https://pih', 'https://no', 'https://nn', 'https://nrm', 'https://nov', 'https://oc', 'https://mhr', 'https://or', 'https://om', 'https://uz', 'https://pa', 'https://pnb', 'https://pap', 'https://ps', 'https://jam', 'https://koi', 'https://km', 'https://pcd', 'https://pms', 'https://tpi', 'https://nds', 'https://pl', 'https://pnt', 'https://pt', 'https://kaa', 'https://crh', 'https://ksh', 'https://ro', 'https://rmy', 'https://rm', 'https://qu', 'https://rue', 'https://ru', 'https://sah', 'https://sat', 'https://sm', 'https://sa', 'https://sc', 'https://sco', 'https://stq', 'https://st', 'https://sq', 'https://scn', 'https://si', 'https://simple', 'https://sd', 'https://sk', 'https://sl', 'https://cu', 'https://szl', 'https://so', 'https://ckb', 'https://srn', 'https://sr', 'https://sh', 'https://fi', 'https://sv', 'https://tl', 'https://ta', 'https://kab', 'https://roa', 'https://tt', 'https://te', 'https://tet', 'https://th', 'https://tg', 'https://chr', 'https://tr', 'https://tk', 'https://tw', 'https://udm', 'https://uk', 'https://ur', 'https://ug', 'https://vec', 'https://vep', 'https://vi', 'https://vo', 'https://fiu', 'https://wa', 'https://zh', 'https://vls', 'https://war', 'https://wo', 'https://wuu', 'https://yi', 'https://yo', 'https://zh', 'https://diq', 'https://zea', 'https://bat', 'https://zh', 'https://www', 'https://foundation', 'https://www', 'https://foundation', 'https://wikimediafoundation']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('\\w*https://\\w+')\n",
    "https = re.findall(pattern, text)\n",
    "\n",
    "print(https)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('<div id=(.+?).*?>.*?<\\div>')\n",
    "pattern = re.compile('/<div id=[\\\"]?class=[\\\"](.*)[\\\"].*>')\n",
    "classes = re.findall(pattern, text)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(digits9, 'b6fdf95b80d5d6ff39ad2cefc21479c91688eeb2', 'Incorrect data', \"Exercise 1.1 is successful\")\n",
    "Test.assertEqualsHashed(https, '3da45b004a94b6b7ebdf95761b95518262cb343a', 'Incorrect data', \"Exercise 1.2 is successful\")\n",
    "Test.assertEqualsHashed(classes, 'cd27dd34998def66a41b74297fee3231cfadc352', 'Incorrect data', \"Exercise 1.3 is successful\")\n",
    "Test.assertEqualsHashed(ids, 'a4de9747bfb87c9b3502ebff54473df128b0011a', 'Incorrect data', \"Exercise 1.4 is successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization, lemmatization, normalization, stop words removing \n",
    "We’re all very familiar with text, since we read and write it every day. We will use text as raw data for the programs we write, programs that manipulate and analyze it in a variety of interesting ways. <br>\n",
    "\n",
    "There are many ways to examine the context of a text apart from simply reading it. \n",
    "\n",
    "Let us consider key methods of text processing, while continuing to study NLTK.\n",
    "Let us examine the text of King James Bible from Gutenberg Corps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[The King James Bible]\\n\\nThe Old Testament of the King James Bible\\n\\nThe First Book of Moses:  Called Genesis\\n\\n\\n1:1 In the beginning God created the heaven and the earth.\\n\\n1:2 And the earth was without form, and void; and darkness was upon\\nthe face of the deep. And the Spirit of God moved upon the face of the\\nwaters.\\n\\n1:3 And God said, Let there be light: and there was light.\\n\\n1:4 And God saw the light, that it was good: and God divided the light\\nfrom the darkness.\\n\\n1:5 And God called the light Da'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "bible_kjv = gutenberg.raw('bible-kjv.txt')\n",
    "bible_kjv[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization: Splitting into words\n",
    "Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. The list of tokens becomes input for further processing such as parsing or text mining. <br>\n",
    "\n",
    "Typically, tokenization occurs at the word level. However, it is sometimes difficult to define what is meant by a \"word\". Often a tokenizer relies on simple heuristics, for example:\n",
    "- Punctuation and whitespace may or may not be included in the resulting list of tokens.\n",
    "- All contiguous strings of alphabetic characters are part of one token; likewise with numbers\n",
    "- Tokens are separated by whitespace characters, such as a space or line break, or by punctuation characters.\n",
    "\n",
    "In languages that use inter-word spaces (such as most that use the Latin alphabet, and most programming languages), this approach is fairly straightforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the simplest method of word splitting based on whitespace: <b>WhitespaceTokenizer</b>. In general, users should use the string `split()` method instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[The', 'King', 'James', 'Bible]', 'The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible', 'The', 'First', 'Book', 'of', 'Moses:', 'Called', 'Genesis', '1:1', 'In', 'the', 'beginning', 'God', 'created']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "bible_kjv_words = WhitespaceTokenizer().tokenize(bible_kjv)\n",
    "print (bible_kjv_words[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 62097), ('and', 38575), ('of', 34445), ('to', 13383), ('And', 12735), ('that', 12451), ('in', 12167), ('shall', 9760), ('he', 9508), ('unto', 8930), ('I', 8708), ('his', 8362), ('a', 7942), ('for', 7140), ('they', 6893), ('be', 6718), ('is', 6695), ('with', 5951), ('not', 5840), ('all', 5238)]\n"
     ]
    }
   ],
   "source": [
    "bible_kjv_words_f_d = nltk.FreqDist(bible_kjv_words)\n",
    "print (bible_kjv_words_f_d.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "freq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = ['Lord','God','Jesus','Israel','say']\n",
    "freq.append([bible_kjv_words_f_d[i] for i in words])\n",
    "plt.bar(range(len(freq[-1])),freq[-1])\n",
    "plt.xticks(range(len(freq[-1])),words, rotation=60) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in the top up mostly insignificant words.<br>\n",
    "\n",
    "More flexible and powerful tool there are Regular-Expression Tokenizers. It allows you to build up its own rules of word splitting, using regular expressions. Let's choose a words consisting of more than 3 characters and only from letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'King', 'James', 'Bible', 'The', 'Old', 'Testament', 'the', 'King', 'James', 'Bible', 'The', 'First', 'Book', 'Moses', 'Called', 'Genesis', 'the', 'beginning', 'God']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[A-Za-z]{3,}')\n",
    "bible_kjv_words = tokenizer.tokenize(bible_kjv)\n",
    "print (bible_kjv_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 62103), ('and', 38847), ('And', 12846), ('that', 12576), ('shall', 9760), ('unto', 8940), ('his', 8385), ('for', 7228), ('they', 6970), ('LORD', 6651), ('him', 6649), ('not', 6553), ('them', 6425), ('with', 5961), ('all', 5426), ('thou', 4890), ('was', 4515), ('thy', 4450), ('which', 4283), ('God', 4115)]\n"
     ]
    }
   ],
   "source": [
    "bible_kjv_words_f_d = nltk.FreqDist(bible_kjv_words)\n",
    "print (bible_kjv_words_f_d.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = ['Lord','God','Jesus','Israel','say']\n",
    "freq.append([bible_kjv_words_f_d[i] for i in words])\n",
    "plt.bar(range(len(freq[-1])),freq[-1])\n",
    "plt.bar(range(len(freq[-2])),freq[-2],color='r',alpha=0.5)\n",
    "plt.xticks(range(len(freq[-1])),words, rotation=60) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1010"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_words_f_d['say']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's compare tokenizers (we will include also a new one [`word_tokenize()`](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize)) with classical `split()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`split()`: ['She', 'looked', 'at', 'her', \"father's\", 'arm-chair.']\n",
      "\n",
      "WhitespaceTokenizer: ['She', 'looked', 'at', 'her', \"father's\", 'arm-chair.']\n",
      "\n",
      "RegexpTokenizer with at least 1-letters words: ['She', 'looked', 'at', 'her', 'father', 's', 'arm', 'chair']\n",
      "\n",
      "RegexpTokenizer with at least 2-letters words: ['She', 'looked', 'at', 'her', 'father', 'arm', 'chair']\n",
      "\n",
      "RegexpTokenizer with at least 3-letters words: ['She', 'looked', 'her', 'father', 'arm', 'chair']\n",
      "\n",
      "word_tokenize(): ['She', 'looked', 'at', 'her', 'father', \"'s\", 'arm-chair', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"She looked at   her father's arm-chair.\"\n",
    "\n",
    "# Apply simple `split` method\n",
    "print (\"`split()`:\", text.split())\n",
    "\n",
    "# Apply WhitespaceTokenizer\n",
    "print (\"\\nWhitespaceTokenizer:\", WhitespaceTokenizer().tokenize(text))\n",
    "\n",
    "# Apply RegexpTokenizer with at least 1-letters words\n",
    "print (\"\\nRegexpTokenizer with at least 1-letters words:\", RegexpTokenizer('[A-Za-z]{1,}').tokenize(text))\n",
    "\n",
    "# Apply RegexpTokenizer with at least 2-letters words\n",
    "print (\"\\nRegexpTokenizer with at least 2-letters words:\", RegexpTokenizer('[A-Za-z]{2,}').tokenize(text))\n",
    "\n",
    "# Apply RegexpTokenizer with at least 3-letters words\n",
    "print (\"\\nRegexpTokenizer with at least 3-letters words:\", RegexpTokenizer('[A-Za-z]{3,}').tokenize(text))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print (\"\\nword_tokenize():\", word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see `nltk` library provides various types for divide sentences into words.\n",
    "\n",
    "This Python library also provides tools for separation of some text into sentences ([`sent_tokenize`](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize), for example).\n",
    "\n",
    "Let's look at an example of it usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n    Mrs. Jones knows that both Mr. Smith and Johann S. Bach should be in the hotel.',\n",
       " \"But this time she didn't find them there.\",\n",
       " \"'Oh!\",\n",
       " \"It's too strange', said she nervously.\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"\"\"\n",
    "    Mrs. Jones knows that both Mr. Smith and Johann S. Bach should be in the hotel. \n",
    "    But this time she didn't find them there. \n",
    "    'Oh! It's too strange', said she nervously.\n",
    "\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the picture is more interesting, but the text still needs to be processed.<br>\n",
    "More information you can see there here <a href=\"http://www.nltk.org/api/nltk.tokenize.html\">nltk.tokenize package</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Exercise 2\n",
    "Here [https://www.cs.cmu.edu/~spok/grimmtmp/016.txt](https://www.cs.cmu.edu/~spok/grimmtmp/016.txt) you can load the text of the \"Cinderella\" fairy tale of brothers Grimm using `requests` library as it was made before. But the extracted text doesn't contain any HTML tags.\n",
    "\n",
    "> Apply `RegexpTokenizer('[A-Za-z]{1,}')` to this text and create a Python dict containing words as keys and its amount in the txt file, for example\n",
    ">```\n",
    "    { \"the\": 15, \"Cinderella\": 23, \"man\": 2, ... }\n",
    "```\n",
    "> Write results to `cinderella_tokens` variable. This approach is common used for text vectorization, that we will consider in the next lesson.\n",
    "\n",
    "><i>Note: FreqDist easily converted to the dictionary using type conversion </i>\n",
    "><pre>\n",
    ">nltk.FreqDist(....)                                  dict(nltk.FreqDist(....) ) \n",
    ">\n",
    "     FreqDist({u'Afterwards': 1,                     {u'Afterwards': 1,\n",
    "          u'And': 16,                                       u'And': 16,\n",
    "          ....                                                     ....\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\tCinderella\\nThe wife of a rich man fell sick, and as she felt that her end\\nwas drawing near, she cal'\n",
      "[('the', 212), ('and', 173), ('her', 80), ('to', 68), ('she', 44), ('had', 36), ('on', 33), ('with', 33), ('you', 31), ('was', 30), ('a', 28), ('cinderella', 26), ('said', 25), ('it', 25), ('into', 25), ('for', 25), ('he', 25), ('s', 24), ('pick', 20), ('that', 19)]\n",
      "{'Cinderella': 3, 'The': 16, 'wife': 4, 'of': 16, 'a': 28, 'rich': 1, 'man': 4, 'fell': 2, 'sick': 1, 'and': 173, 'as': 9, 'she': 44, 'felt': 1, 'that': 19, 'her': 80, 'end': 1, 'was': 30, 'drawing': 1, 'near': 2, 'called': 5, 'only': 3, 'daughter': 2, 'to': 68, 'bedside': 1, 'said': 25, 'dear': 1, 'child': 2, 'be': 13, 'good': 8, 'pious': 2, 'then': 6, 'the': 212, 'God': 1, 'will': 7, 'always': 3, 'protect': 1, 'you': 31, 'I': 4, 'look': 2, 'down': 16, 'on': 33, 'from': 12, 'heaven': 1, 'Thereupon': 1, 'closed': 1, 'eyes': 1, 'departed': 1, 'Every': 1, 'day': 5, 'maiden': 12, 'went': 19, 'out': 14, 'mother': 17, 's': 24, 'grave': 7, 'wept': 5, 'remained': 3, 'When': 9, 'winter': 1, 'came': 16, 'snow': 1, 'spread': 1, 'white': 6, 'sheet': 1, 'over': 5, 'by': 9, 'time': 2, 'spring': 1, 'sun': 1, 'had': 36, 'drawn': 1, 'it': 25, 'off': 9, 'again': 9, 'taken': 4, 'another': 1, 'woman': 1, 'brought': 2, 'with': 33, 'into': 25, 'house': 7, 'two': 17, 'daughters': 5, 'who': 4, 'were': 9, 'beautiful': 10, 'fair': 2, 'face': 3, 'but': 12, 'vile': 1, 'black': 1, 'heart': 1, 'Now': 1, 'began': 6, 'bad': 3, 'for': 25, 'poor': 1, 'step': 16, 'Is': 1, 'stupid': 1, 'goose': 1, 'sit': 2, 'in': 19, 'parlor': 1, 'us': 5, 'they': 18, 'He': 6, 'wants': 1, 'eat': 1, 'bread': 1, 'must': 2, 'earn': 1, 'Out': 1, 'kitchen': 7, 'wench': 2, 'They': 3, 'took': 10, 'pretty': 2, 'clothes': 6, 'away': 8, 'put': 5, 'an': 4, 'old': 2, 'grey': 3, 'bedgown': 1, 'gave': 7, 'wooden': 2, 'shoes': 3, 'Just': 1, 'at': 18, 'proud': 2, 'princess': 2, 'how': 5, 'decked': 1, 'is': 12, 'cried': 8, 'laughed': 2, 'led': 1, 'There': 1, 'do': 3, 'hard': 1, 'work': 1, 'morning': 2, 'till': 3, 'night': 1, 'get': 3, 'up': 5, 'before': 4, 'daybreak': 1, 'carry': 1, 'water': 1, 'light': 1, 'fires': 1, 'cook': 1, 'wash': 1, 'Besides': 1, 'this': 10, 'sisters': 8, 'did': 3, 'every': 2, 'imaginable': 1, 'injury': 1, 'mocked': 1, 'emptied': 3, 'peas': 1, 'lentils': 5, 'ashes': 10, 'so': 6, 'forced': 3, 'pick': 20, 'them': 9, 'In': 1, 'evening': 4, 'when': 13, 'worked': 1, 'weary': 1, 'no': 17, 'bed': 1, 'go': 14, 'sleep': 1, 'hearth': 1, 'cinders': 1, 'And': 16, 'account': 1, 'looked': 5, 'dusty': 1, 'dirty': 3, 'cinderella': 26, 'It': 2, 'happened': 2, 'father': 5, 'once': 4, 'going': 2, 'he': 25, 'asked': 1, 'his': 14, 'what': 3, 'should': 2, 'bring': 2, 'back': 6, 'Beautiful': 1, 'dresses': 2, 'one': 17, 'pearls': 2, 'jewels': 2, 'second': 1, 'have': 10, 'Father': 1, 'break': 1, 'me': 8, 'first': 2, 'branch': 4, 'which': 9, 'knocks': 1, 'against': 2, 'your': 3, 'hat': 2, 'way': 2, 'home': 9, 'So': 2, 'bought': 1, 'riding': 1, 'through': 3, 'green': 1, 'thicket': 1, 'hazel': 9, 'twig': 1, 'brushed': 1, 'him': 12, 'knocked': 1, 'Then': 14, 'broke': 1, 'reached': 1, 'things': 1, 'wished': 5, 'bush': 1, 'thanked': 1, 'planted': 1, 'much': 3, 'tears': 1, 'watered': 1, 'grew': 1, 'became': 2, 'handsome': 1, 'tree': 17, 'Thrice': 1, 'sat': 3, 'beneath': 6, 'prayed': 1, 'little': 9, 'bird': 7, 'if': 5, 'expressed': 1, 'wish': 1, 'threw': 4, 'however': 7, 'king': 17, 'orders': 1, 'festival': 5, 'last': 3, 'three': 1, 'days': 2, 'all': 13, 'young': 1, 'girls': 1, 'country': 1, 'invited': 3, 'order': 1, 'son': 16, 'might': 3, 'choose': 1, 'himself': 1, 'bride': 10, 'heard': 1, 'too': 8, 'appear': 1, 'among': 3, 'number': 1, 'delighted': 2, 'comb': 1, 'our': 3, 'hair': 1, 'brush': 1, 'fasten': 1, 'buckles': 1, 'we': 1, 'are': 4, 'wedding': 5, 'palace': 1, 'obeyed': 1, 'because': 1, 'would': 6, 'liked': 1, 'dance': 6, 'begged': 1, 'allow': 1, 'You': 4, 'covered': 1, 'dust': 1, 'dirt': 2, 'yet': 2, 'As': 3, 'asking': 1, 'dish': 3, 'picked': 2, 'hours': 1, 'shall': 3, 'door': 2, 'garden': 3, 'tame': 2, 'pigeons': 9, 'turtle': 4, 'doves': 6, 'birds': 4, 'sky': 4, 'come': 2, 'help': 3, 'pot': 2, 'crop': 2, 'window': 2, 'afterwards': 2, 'whirring': 2, 'crowding': 2, 'alighted': 2, 'amongst': 4, 'nodded': 2, 'their': 4, 'heads': 2, 'rest': 1, 'also': 3, 'gathered': 2, 'grains': 1, 'Hardly': 1, 'hour': 3, 'passed': 3, 'finished': 2, 'flew': 2, 'girl': 1, 'glad': 2, 'believed': 3, 'now': 4, 'allowed': 1, 'But': 6, 'can': 5, 'not': 10, 'dishes': 3, 'thought': 5, 'herself': 4, 'most': 2, 'certainly': 1, 'cannot': 4, 'length': 1, 'others': 2, 'seeds': 1, 'half': 1, 'already': 1, 'We': 1, 'ashamed': 1, 'On': 2, 'turned': 3, 'hurried': 1, 'shiver': 3, 'quiver': 3, 'silver': 5, 'gold': 4, 'throw': 3, 'dress': 8, 'slippers': 2, 'embroidered': 1, 'silk': 1, 'She': 5, 'speed': 1, 'Her': 1, 'know': 2, 'foreign': 1, 'golden': 5, 'never': 2, 'sitting': 2, 'picking': 1, 'prince': 1, 'approached': 1, 'hand': 3, 'danced': 5, 'other': 6, 'let': 1, 'loose': 1, 'any': 3, 'else': 1, 'invite': 1, 'my': 7, 'partner': 3, 'wanted': 4, 'bear': 1, 'company': 1, 'see': 2, 'whom': 1, 'belonged': 1, 'escaped': 3, 'sprang': 2, 'pigeon': 4, 'waited': 3, 'until': 3, 'told': 1, 'unknown': 2, 'leapt': 1, 'axe': 2, 'pickaxe': 1, 'hew': 1, 'pieces': 1, 'inside': 1, 'got': 3, 'lay': 2, 'dim': 1, 'oil': 1, 'lamp': 1, 'burning': 1, 'mantle': 1, 'piece': 1, 'jumped': 2, 'quickly': 2, 'run': 1, 'there': 8, 'laid': 1, 'seated': 2, 'gown': 2, 'Next': 2, 'afresh': 1, 'parents': 2, 'gone': 3, 'more': 6, 'than': 2, 'preceding': 1, 'appeared': 1, 'astonished': 1, 'beauty': 1, 'instantly': 1, 'leave': 2, 'followed': 1, 'behind': 2, 'Therein': 1, 'stood': 2, 'tall': 1, 'hung': 1, 'magnificent': 2, 'pears': 1, 'clambered': 1, 'nimbly': 1, 'between': 1, 'branches': 1, 'like': 2, 'squirrel': 1, 'where': 1, 'has': 2, 'believe': 1, 'climbed': 1, 'pear': 1, 'cut': 5, 'usual': 1, 'side': 2, 'third': 1, 'splendid': 1, 'knew': 1, 'speak': 1, 'astonishment': 1, 'anxious': 1, 'could': 2, 'follow': 1, 'employed': 1, 'ruse': 1, 'caused': 1, 'whole': 1, 'staircase': 1, 'smeared': 1, 'pitch': 1, 'ran': 1, 'left': 5, 'slipper': 3, 'stuck': 1, 'small': 5, 'dainty': 1, 'whose': 1, 'foot': 8, 'fits': 1, 'feet': 1, 'eldest': 1, 'shoe': 15, 'room': 1, 'try': 1, 'big': 1, 'toe': 3, 'knife': 2, 'queen': 2, 'need': 2, 'swallowed': 2, 'pain': 2, 'horse': 5, 'rode': 3, 'obliged': 1, 'pass': 1, 'turn': 6, 'peep': 6, 'blood': 5, 'within': 2, 'true': 5, 'waits': 2, 'saw': 2, 'trickling': 1, 'round': 1, 'false': 3, 'sister': 1, 'chamber': 1, 'toes': 1, 'safely': 1, 'heel': 3, 'large': 1, 'bit': 2, 'running': 1, 'stained': 1, 'stocking': 1, 'quite': 1, 'red': 1, 'This': 1, 'right': 4, 'No': 1, 'still': 1, 'stunted': 1, 'late': 1, 'possibly': 1, 'send': 1, 'answered': 1, 'oh': 1, 'show': 1, 'absolutely': 1, 'insisted': 1, 'washed': 1, 'hands': 1, 'clean': 1, 'bowed': 1, 'stool': 1, 'drew': 1, 'heavy': 1, 'fitted': 1, 'glove': 1, 'rose': 1, 'recognized': 1, 'horrified': 1, 'pale': 1, 'rage': 1, 'rides': 1, 'flying': 1, 'placed': 1, 'themselves': 1, 'shoulders': 1, 'celebrated': 1, 'favor': 1, 'share': 1, 'fortune': 1, 'betrothed': 1, 'couple': 1, 'church': 1, 'elder': 2, 'younger': 2, 'pecked': 2, 'eye': 2, 'each': 2, 'Afterwards': 1, 'thus': 1, 'wickedness': 1, 'falsehood': 1, 'punished': 1, 'blindness': 1}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.get('https://www.cs.cmu.edu/~spok/grimmtmp/016.txt')\n",
    "text = r.text.encode('utf-8')\n",
    "print(text[:100])\n",
    "text = r.text\n",
    "\n",
    "tokenizer = RegexpTokenizer('[A-Za-z]{1,}')\n",
    "text = tokenizer.tokenize(text)\n",
    "extfind = nltk.FreqDist(text)\n",
    "print (extfind.most_common(20))\n",
    "\n",
    "cinderella_tokens = dict(nltk.FreqDist(text))\n",
    "print(cinderella_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(cinderella_tokens, '82684208617d5791ac8498699067539ff4b6d131', 'Incorrect data', \"Exercise 2 is successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "We saw how to get words from the text, but as long as our program does not understand similar words. Stemming and lemmatization are used to solve this problem.\n",
    "\n",
    "In linguistic morphology and information retrieval, **stemming** is the process for reducing inflected (or sometimes derived) words to their stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation\n",
    "\n",
    "**Lemmatisation** (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
    "\n",
    "In computational linguistics, lemmatisation is the algorithmic process of determining the lemma for a given word. Since the process may involve complex tasks such as understanding context and determining the part of speech of a word in a sentence (requiring, for example, knowledge of the grammar of a language) it can be a hard task to implement a lemmatiser for a new language.\n",
    "\n",
    "In many languages, words appear in several inflected forms. For example, in English, the verb ‘to walk’ may appear as ‘walk’, ‘walked’, ‘walks’, ‘walking’. The base form, ‘walk’, that one might look up in a dictionary, is called the lemma for the word. The combination of the base form with the part of speech is often called the lexeme of the word.\n",
    "\n",
    "Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n",
    " \n",
    "Both stemming and lemmatization are presented in [`nltk.stem`](http://www.nltk.org/api/nltk.stem.html) module.\n",
    "\n",
    "NLTK provides several famous stemmers interfaces, such as [Porter Stemmer](http://tartarus.org/martin/PorterStemmer/), [Lancaster Stemmer](http://www.lancaster.ac.uk/scc/), [Snowball Stemmer](http://snowball.tartarus.org/) and etc. In NLTK, using those stemmers is very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmers for \"said\":\n",
      "said said said\n",
      "\n",
      "stemmers for \"saying\":\n",
      "say say say\n",
      "\n",
      "stemmers for \"perfectly\":\n",
      "perfectli perfect perfect\n",
      "\n",
      "stemmers for \"faster\":\n",
      "faster fast faster\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "print ('stemmers for \"said\":')\n",
    "print (porter_stemmer.stem(\"said\"), lancaster_stemmer.stem(\"said\"), snowball_stemmer.stem(\"said\"))\n",
    "\n",
    "print ('\\nstemmers for \"saying\":')\n",
    "print (porter_stemmer.stem(\"saying\"), lancaster_stemmer.stem(\"saying\"), snowball_stemmer.stem(\"saying\"))\n",
    "\n",
    "print ('\\nstemmers for \"perfectly\":')\n",
    "print (porter_stemmer.stem(\"perfectly\"), lancaster_stemmer.stem(\"perfectly\"), snowball_stemmer.stem(\"perfectly\"))\n",
    "\n",
    "print ('\\nstemmers for \"faster\":')\n",
    "print (porter_stemmer.stem(\"faster\"), lancaster_stemmer.stem(\"faster\"), snowball_stemmer.stem(\"faster\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WordNet Lemmatizer](http://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer) returns the input word unchanged if it cannot be found in WordNet. Let's look how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs -> dog\n",
      "oxen -> ox\n",
      "mens -> men\n",
      "\n",
      "are -> are\n",
      "is -> is\n",
      "being -> being\n",
      "\n",
      "are -> be\n",
      "is -> be\n",
      "being -> be\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print (\"dogs ->\", lemmatizer.lemmatize('dogs'))\n",
    "print (\"oxen ->\", lemmatizer.lemmatize('oxen'))\n",
    "print (\"mens ->\", lemmatizer.lemmatize(\"mens\"))\n",
    "\n",
    "# Below lines return the same words (but we expect to see \"be\")\n",
    "print (\"\\nare ->\", lemmatizer.lemmatize('are'))\n",
    "print (\"is ->\", lemmatizer.lemmatize('is'))\n",
    "print (\"being ->\", lemmatizer.lemmatize('being'))\n",
    "\n",
    "# You would note that the “are” and “is” lemmatize results are not “be”, \n",
    "# that’s because the lemmatize method default 'pos' argument is “n”:\n",
    "# We need to define the part of speech\n",
    "print (\"\\nare ->\", lemmatizer.lemmatize('are', pos='v'))\n",
    "print (\"is ->\", lemmatizer.lemmatize('is', pos='v'))\n",
    "print (\"being ->\", lemmatizer.lemmatize('being', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is using `wordnet` construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n",
      "say\n",
      "say\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "print (lemmatizer.lemmatize('said', wordnet.VERB))\n",
    "print (lemmatizer.lemmatize('says', wordnet.VERB))\n",
    "print (lemmatizer.lemmatize('say', wordnet.VERB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second optional parameter specifies what dictionary to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 62103), ('and', 38847), ('And', 12846), ('that', 12576), ('be', 10808), ('shall', 9760), ('unto', 8940), ('his', 8385), ('for', 7228), ('they', 6970), ('LORD', 6651), ('him', 6649), ('not', 6553), ('say', 6426), ('them', 6425), ('have', 6042), ('with', 5961), ('all', 5426), ('thou', 4890), ('thy', 4450)]\n"
     ]
    }
   ],
   "source": [
    "bible_kjv_words_l = [lemmatizer.lemmatize(w, wordnet.VERB) for w in bible_kjv_words]\n",
    "bible_kjv_words_f_d = nltk.FreqDist(bible_kjv_words_l)\n",
    "print (bible_kjv_words_f_d.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = ['Lord','God','Jesus','Israel','say']\n",
    "freq.append([bible_kjv_words_f_d[i] for i in words])\n",
    "plt.bar(range(len(freq[-1])),freq[-1])\n",
    "plt.bar(range(len(freq[-2])),freq[-2],color='r',alpha=0.5)\n",
    "plt.xticks(range(len(freq[-1])),words, rotation=60) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6426"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_words_f_d['say']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have after processing more clear that the word say is found in various forms in 5416  times  against 1010 times what we found earlier.\n",
    "<br>\n",
    "More here <a href=\"http://www.nltk.org/api/nltk.stem.html\">nltk.stem package</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Exercise 3\n",
    "In the previous exercise we have count words in the [\"Cinderella\" fairy tale](https://www.cs.cmu.edu/~spok/grimmtmp/016.txt). But the resulting dictionary contains many similar words (plural, no main verbs form, etc.). Let's confine ourselves by consideration of plural (men -> man, girls -> girl, etc.) and main verbs form (saying -> say, said -> say, etc.) and apply `WordNetLemmatizer` to the previous results. Thus, you should obtain another dictionary (call it `cinderella_lemma`) with updated keyses and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n",
      "b'\\tCinderella\\nThe wife of a rich man fell sick, and as she felt that her end\\nwas drawing near, she cal'\n",
      "{'Cinderella': 3, 'The': 16, 'wife': 4, 'of': 16, 'a': 37, 'rich': 1, 'man': 4, 'fell': 2, 'sick': 1, ',': 260, 'and': 173, 'she': 44, 'felt': 1, 'that': 19, 'her': 80, 'end': 1, 'be': 68, 'draw': 1, 'near': 2, 'call': 5, 'only': 3, 'daughter': 4, 'to': 68, 'bedside': 1, 'say': 25, 'dear': 1, 'child': 1, 'good': 8, 'pious': 2, 'then': 6, 'the': 212, 'God': 1, 'will': 7, 'always': 3, 'protect': 1, 'you': 31, 'I': 4, 'look': 7, 'down': 16, 'on': 33, 'from': 12, 'heaven': 1, '.': 111, 'Thereupon': 1, 'close': 1, 'eye': 3, 'depart': 1, 'Every': 1, 'day': 7, 'maiden': 12, 'go': 38, 'out': 14, 'mother': 6, \"'s\": 20, 'grave': 7, 'wept': 5, 'remain': 3, 'When': 9, 'winter': 1, 'come': 18, 'snow': 1, 'spread': 1, 'white': 6, 'sheet': 1, 'over': 5, 'by': 9, 'time': 2, 'spring': 1, 'sun': 1, 'have': 48, 'drawn': 1, 'it': 25, 'off': 9, 'again': 9, 'take': 14, 'another': 1, 'woman': 1, 'brought': 2, 'with': 33, 'into': 25, 'house': 3, 'two': 17, 'who': 4, 'beautiful': 10, 'fair': 2, 'face': 3, 'but': 12, 'vile': 1, 'black': 1, 'heart': 1, 'Now': 1, 'begin': 6, 'bad': 3, 'for': 25, 'poor': 1, 'step-child': 1, 'Is': 1, 'stupid': 1, 'goose': 1, 'sit': 4, 'in': 19, 'parlor': 1, 'u': 5, 'they': 18, 'He': 6, 'want': 5, 'eat': 1, 'bread': 1, 'must': 2, 'earn': 1, 'Out': 1, 'kitchen-wench': 2, 'They': 3, 'pretty': 2, 'clothes': 6, 'away': 8, 'put': 5, 'an': 4, 'old': 2, 'grey': 3, 'bedgown': 1, 'give': 7, 'wooden': 2, 'shoe': 18, 'Just': 1, 'at': 18, 'proud': 2, 'princess': 2, 'how': 5, 'deck': 1, 'cry': 8, 'laugh': 2, 'lead': 1, 'kitchen': 4, 'There': 1, 'do': 6, 'hard': 1, 'work': 2, 'morning': 2, 'till': 3, 'night': 1, 'get': 6, 'up': 5, 'before': 4, 'daybreak': 1, 'carry': 1, 'water': 2, 'light': 1, 'fire': 1, 'cook': 1, 'wash': 2, 'Besides': 1, 'this': 10, 'sister': 6, 'every': 2, 'imaginable': 1, 'injury': 1, '-': 7, 'mocked': 1, 'empty': 3, 'pea': 1, 'lentil': 5, 'ash': 10, 'so': 6, 'force': 3, 'pick': 21, 'them': 9, 'In': 1, 'even': 4, 'when': 13, 'weary': 1, 'no': 17, 'bed': 1, 'sleep': 1, 'hearth': 1, 'cinder': 1, 'And': 16, 'account': 1, 'dusty': 1, 'dirty': 3, 'cinderella': 26, 'It': 2, 'happen': 2, 'father': 5, 'once': 4, 'he': 25, 'ask': 2, 'his': 14, 'step-daughters': 3, 'what': 3, 'should': 2, 'bring': 2, 'back': 4, 'Beautiful': 1, 'dress': 10, 'one': 17, 'pearl': 2, 'jewel': 2, 'second': 1, 'Father': 1, 'break': 1, 'me': 8, 'first': 2, 'branch': 5, 'which': 9, 'knock': 2, 'against': 2, 'your': 3, 'hat': 2, 'way': 2, 'home': 9, 'So': 2, 'bought': 1, 'rid': 1, 'through': 3, 'green': 1, 'thicket': 1, 'hazel': 1, 'twig': 1, 'brush': 2, 'him': 12, 'Then': 14, 'broke': 1, 'reach': 1, 'thing': 1, 'wish': 6, 'hazel-bush': 1, 'thanked': 1, \"mother's\": 2, 'plant': 1, 'much': 3, 'tear': 1, 'grow': 1, 'become': 2, 'handsome': 1, 'tree': 9, 'Thrice': 1, 'sat': 3, 'beneath': 6, 'prayed': 1, 'little': 9, 'bird': 11, 'if': 5, 'express': 1, 'threw': 4, 'however': 7, 'king': 15, 'order': 2, 'festival': 5, 'last': 3, 'three': 1, 'all': 13, 'young': 3, 'girl': 2, 'country': 1, 'invite': 4, 'son': 16, 'might': 3, 'choose': 1, 'himself': 1, 'bride': 10, 'step-sisters': 3, 'heard': 1, 'too': 8, 'appear': 2, 'among': 3, 'number': 1, 'delighted': 2, 'comb': 1, 'our': 3, 'hair': 1, 'fasten': 1, 'buckle': 1, 'we': 1, 'wedding': 5, 'palace': 1, 'obeyed': 1, 'because': 1, 'would': 6, 'like': 3, 'dance': 11, 'beg': 1, 'step-mother': 9, 'allow': 2, 'You': 4, 'cover': 1, 'dust': 1, 'dirt': 2, 'yet': 2, 'As': 3, 'dish': 6, 'picked': 2, 'hour': 4, 'shall': 3, 'back-door': 2, 'garden': 3, 'tame': 2, 'pigeon': 9, 'turtle-doves': 4, 'sky': 4, 'help': 3, 'pot': 2, 'crop': 2, 'window': 1, 'afterwards': 2, 'whir': 2, 'crowd': 2, 'alight': 2, 'amongst': 4, 'nod': 2, 'their': 4, 'head': 2, 'rest': 1, 'also': 3, 'gather': 2, 'grain': 1, 'Hardly': 1, 'pass': 3, 'finish': 2, 'flew': 2, 'glad': 2, 'believe': 4, 'now': 4, 'But': 6, 'can': 9, 'not': 14, 'thought': 5, 'herself': 4, 'most': 2, 'certainly': 1, 'kitchen-window': 1, 'length': 1, 'dove': 2, 'others': 2, 'seed': 1, 'half': 1, 'already': 1, 'We': 1, 'ashamed': 1, 'On': 2, 'turn': 9, 'hurry': 1, 'hazel-tree': 7, 'shiver': 3, 'quiver': 3, 'silver': 5, 'gold': 4, 'throw': 3, 'slipper': 5, 'embroider': 1, 'silk': 1, 'She': 5, 'speed': 1, 'Her': 1, 'know': 2, 'foreign': 1, 'golden': 5, 'never': 2, 'prince': 1, 'approach': 1, 'hand': 4, 'other': 6, 'let': 1, 'loose': 1, 'any': 3, 'else': 1, 'my': 7, 'partner': 3, 'bear': 1, 'company': 1, 'see': 2, 'whom': 1, 'belong': 1, 'escape': 3, 'sprang': 2, 'pigeon-house': 4, 'wait': 5, 'until': 3, 'told': 1, 'unknown': 2, 'leapt': 1, 'axe': 2, 'pickaxe': 1, 'hew': 1, 'piece': 1, 'inside': 1, 'lay': 2, 'dim': 1, 'oil-lamp': 1, 'burning': 1, 'mantle-piece': 1, 'jumped': 2, 'quickly': 2, 'run': 2, 'there': 8, 'laid': 1, 'seat': 2, 'gown': 2, 'Next': 2, 'afresh': 1, 'parent': 2, 'more': 6, 'than': 2, 'precede': 1, 'astonish': 1, 'beauty': 1, \"king's\": 2, 'instantly': 1, 'leave': 2, 'follow': 2, 'behind': 2, 'Therein': 1, 'stood': 2, 'tall': 1, 'hung': 1, 'magnificent': 2, 'pear': 1, 'clamber': 1, 'nimbly': 1, 'between': 1, 'squirrel': 1, 'where': 1, 'climbed': 1, 'pear-tree': 1, 'cut': 5, 'usual': 1, 'side': 2, 'third': 1, 'splendid': 1, 'knew': 1, 'speak': 1, 'astonishment': 1, 'anxious': 1, 'could': 2, 'employ': 1, 'ruse': 1, 'cause': 1, 'whole': 1, 'staircase': 1, 'smear': 1, 'pitch': 1, 'ran': 1, 'left': 5, 'stuck': 1, 'small': 5, 'dainty': 1, 'whose': 1, 'foot': 9, 'fit': 2, 'eldest': 1, 'room': 1, 'try': 1, 'big': 1, 'toe': 4, 'knife': 2, 'queen': 2, 'need': 2, 'swallow': 2, 'pain': 2, 'horse': 5, 'rode': 3, 'oblige': 1, 'pas': 1, 'peep': 6, 'blood': 5, 'within': 2, 'true': 5, 'saw': 2, 'trickle': 1, 'round': 1, 'false': 3, 'chamber': 1, 'safely': 1, 'heel': 3, 'large': 1, 'bit': 2, 'stain': 1, 'stock': 1, 'quite': 1, 'red': 1, 'This': 1, 'right': 4, 'No': 1, 'still': 1, 'stunt': 1, 'late': 1, 'possibly': 1, 'send': 1, 'answer': 1, 'oh': 1, 'show': 1, 'absolutely': 1, 'insist': 1, 'clean': 1, 'bow': 1, 'stool': 1, 'drew': 1, 'heavy': 1, 'glove': 1, 'rise': 1, 'recognize': 1, 'horrify': 1, 'pale': 1, 'rage': 1, 'ride': 1, 'fly': 1, 'place': 1, 'themselves': 1, 'shoulder': 1, 'celebrate': 1, 'favor': 1, 'share': 1, 'fortune': 1, 'betroth': 1, 'couple': 1, 'church': 1, 'elder': 2, 'pecked': 2, 'each': 2, 'Afterwards': 1, 'thus': 1, 'wickedness': 1, 'falsehood': 1, 'punish': 1, 'blindness': 1}\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 2. Lemmatize Single Word with the appropriate POS tag\n",
    "word = 'feet'\n",
    "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
    "\n",
    "import requests\n",
    "r = requests.get('https://www.cs.cmu.edu/~spok/grimmtmp/016.txt')\n",
    "text = r.text.encode('utf-8')\n",
    "print(text[:100])\n",
    "text = r.text\n",
    "\n",
    "cinderella_lemma = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n",
    "cinderella_lemma = dict(nltk.FreqDist(cinderella_lemma))\n",
    "print(cinderella_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(cinderella_lemma, '089151bfb8893734473f9b94e4370bf345dc77ad', 'Incorrect data', \"Exercise 3 is successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Normalization - bringing the words to the general view. The human brain is easy to recognize different forms of the same word, but it is more difficult for a computer. Any change, whether it be a register or an additional letter to the computer it will be two completely different words. It is difficult to analyze. \n",
    "<br>The easiest method of normalization have to bring all the words to one register, for example to lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'king', 'james', 'bible', 'the', 'old', 'testament', 'the', 'king', 'james', 'bible', 'the', 'first', 'book', 'moses', 'called', 'genesis', 'the', 'beginning', 'god']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[A-Za-z]{3,}')\n",
    "bible_kjv_words = tokenizer.tokenize(bible_kjv.lower())\n",
    "print (bible_kjv_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 70573), ('the', 62103), ('and', 38847), ('of', 34480), ('.', 26202), ('be', 25373), ('to', 13396), ('And', 12846), (':', 12706), ('that', 12576), ('in', 12331), (';', 10139), ('shall', 9760), ('he', 9665), ('unto', 8940), ('I', 8854), ('his', 8385), ('a', 7943), ('for', 7228), ('they', 6970)]\n"
     ]
    }
   ],
   "source": [
    "bible_kjv_words_l = [lemmatizer.lemmatize(w,wordnet.VERB) for w in bible_kjv_words]\n",
    "bible_kjv_words_f_d = nltk.FreqDist(bible_kjv_words_l)\n",
    "print (bible_kjv_words_f_d.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = ['lord','god','jesus','israel','say']\n",
    "freq.append([bible_kjv_words_f_d[i] for i in words])\n",
    "plt.bar(range(len(freq[-1])),freq[-1])\n",
    "plt.bar(range(len(freq[-2])),freq[-2],color='r',alpha=0.5)\n",
    "plt.xticks(range(len(freq[-1])),words, rotation=60) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6531, 4472)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_words_f_d['say'],bible_kjv_words_f_d['god']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found more words to say, has also increased the number we found references to the word lord and god.<br>\n",
    "\n",
    "Let us consider method of reduction to a single form in a more advanced form, using the pattern unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lord'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pattern.en import singularize\n",
    "singularize('lord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 64023), ('and', 51696), ('that', 12912), ('be', 10869), ('shall', 9838), ('unto', 8997), ('for', 8971), ('hi', 8473), ('lord', 8006), ('they', 7376), ('him', 6659), ('not', 6596), ('say', 6531), ('them', 6430), ('have', 6124), ('with', 6015), ('all', 5620), ('thou', 5474), ('god', 4716), ('thy', 4600)]\n"
     ]
    }
   ],
   "source": [
    "bible_kjv_words_l = [singularize(w) for w in bible_kjv_words_l]\n",
    "bible_kjv_words_f_d = nltk.FreqDist(bible_kjv_words_l)\n",
    "print (bible_kjv_words_f_d.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6531, 4716)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_words_f_d['say'],bible_kjv_words_f_d['god']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = ['lord','god','jesu','israel','say']\n",
    "freq.append([bible_kjv_words_f_d[i] for i in words])\n",
    "plt.bar(range(len(freq[-1])),freq[-1])\n",
    "plt.bar(range(len(freq[-2])),freq[-2],color='r',alpha=0.5)\n",
    "plt.xticks(range(len(freq[-1])),words, rotation=60) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of words to say not changed, but we found more instances of the word god."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More here <a href=\"http://www.clips.ua.ac.be/pattern\">Pattern</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how normalization helps us to better handle similar words. But we are also interested to know the key words of the text. As we have seen from the ranking (top 20), the top most service (stop) words, that we are not interested. Let us consider two tools to remove stop words already known to us `nltk` and specialized <a href=\"https://pypi.python.org/pypi/stop-words\">`stop_words`</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'king', 'james', 'bible', 'the', 'old', 'testament', 'of', 'the', 'king', 'james', 'bible', 'the', 'first', 'book', 'of', 'moses', 'called', 'genesis', 'in']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[A-Za-z]{2,}')\n",
    "bible_kjv_words = tokenizer.tokenize(bible_kjv.lower())\n",
    "print (bible_kjv_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "bible_kjv_words_without_s_w = [w for w in bible_kjv_words if w not in(stop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('shall', 9838), ('unto', 8997), ('lord', 7964), ('thou', 5474), ('thy', 4600), ('god', 4472), ('said', 3999), ('ye', 3983), ('thee', 3827), ('upon', 2748), ('man', 2735), ('israel', 2575), ('king', 2543), ('son', 2392), ('hath', 2264), ('people', 2143), ('came', 2093), ('house', 2024), ('come', 1971), ('one', 1969)]\n"
     ]
    }
   ],
   "source": [
    "bible_kjv_words_f_d = nltk.FreqDist(bible_kjv_words_without_s_w)\n",
    "print (bible_kjv_words_f_d.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('english')\n",
    "bible_kjv_words_without_s_w = [w for w in bible_kjv_words if w not in(stop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('shall', 9838), ('unto', 8997), ('lord', 7964), ('thou', 5474), ('thy', 4600), ('god', 4472), ('said', 3999), ('ye', 3983), ('will', 3836), ('thee', 3827), ('upon', 2748), ('man', 2735), ('israel', 2575), ('king', 2543), ('son', 2392), ('hath', 2264), ('people', 2143), ('came', 2093), ('house', 2024), ('come', 1971)]\n"
     ]
    }
   ],
   "source": [
    "bible_kjv_words_f_d = nltk.FreqDist(bible_kjv_words_without_s_w)\n",
    "print (bible_kjv_words_f_d.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are still not very meaningful words, but more clearly visible theme.\n",
    "As we can see the work of these instruments is similar, although the first has a larger stock of stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has only 2,400 stopwords for 11 languages, package stop-words has stopwords only for 16 languages, but a lot more languages. In addition, the situation can happen when there is a stop word for the current text. In this case, we can create your own list of stop words, and to use it.\n",
    "For example, we have the Latin text. Load stop words from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ab', 'ac', 'ad', 'at', 'atque', 'aut', 'autem', 'cum', 'de', 'dum', 'e', 'erant', 'erat', 'est', 'et', 'etiam', 'ex', 'haec', 'hic', 'hoc', 'in', 'ita', 'me', 'nec', 'neque', 'non', 'per', 'qua', 'quae', 'quam', 'qui', 'quibus', 'quidem', 'quo', 'quod', 're', 'rebus', 'rem', 'res', 'sed', 'si', 'sic', 'sunt', 'tamen', 'tandem', 'te', 'ut', 'vel']\n"
     ]
    }
   ],
   "source": [
    "stop_words = open('la.txt').read().split(',')\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common from all words:\n",
      " [('ad', 6), ('et', 5), ('a', 4), ('finibus', 4), ('est', 3), ('in', 3), ('belgae', 3), ('gallos', 3), ('ab', 3), ('garumna', 3), ('flumine', 3), ('incolunt', 2), ('qui', 2), ('lingua', 2), ('inter', 2), ('flumen', 2), ('sunt', 2), ('quod', 2), ('atque', 2), ('eos', 2)]\n",
      "Most common without stop-words:\n",
      " [('finibus', 4), ('belgae', 3), ('gallos', 3), ('garumna', 3), ('flumine', 3), ('incolunt', 2), ('lingua', 2), ('inter', 2), ('flumen', 2), ('eos', 2), ('pertinent', 2), ('germanis', 2), ('rhenum', 2), ('bellum', 2), ('gerunt', 2), ('eorum', 2), ('septentriones', 2), ('partem', 2), ('gallia', 1), ('omnis', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae,\n",
    "nostra Galli appellantur. Hi omnes lingua, institutis, legibus inter se differunt. Gallos ab Aquitanis Garumna flumen, a Belgis\n",
    "Matrona et Sequana dividit. Horum omnium fortissimi sunt Belgae, propterea quod a cultu atque humanitate provinciae longissime \n",
    "absunt, minimeque ad eos mercatores saepe commeant atque ea quae ad effeminandos animos pertinent important, proximique sunt \n",
    "Germanis, qui trans Rhenum incolunt, quibuscum continenter bellum gerunt. Qua de causa Helvetii quoque reliquos Gallos virtute \n",
    "praecedunt, quod fere cotidianis proeliis cum Germanis contendunt, cum aut suis finibus eos prohibent aut ipsi in eorum finibus \n",
    "bellum gerunt. Eorum una, pars, quam Gallos obtinere dictum est, initium capit a flumine Rhodano, continetur Garumna flumine, \n",
    "Oceano, finibus Belgarum, attingit etiam ab Sequanis et Helvetiis flumen Rhenum, vergit ad septentriones. Belgae ab extremis \n",
    "Galliae finibus oriuntur, pertinent ad inferiorem partem fluminis Rheni, spectant in septentrionem et orientem solem. Aquitania \n",
    "a Garumna flumine ad Pyrenaeos montes et eam partem Oceani quae est ad Hispaniam pertinet; spectat inter occasum solis et \n",
    "septentriones.\"\"\"\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer('[A-Za-z]+')\n",
    "text_words = tokenizer.tokenize(text.lower())\n",
    "\n",
    "text_words_clear = [w for w in text_words if not(w in stop_words)]\n",
    "print ('Most common from all words:\\n',nltk.FreqDist(text_words).most_common(20))\n",
    "print ('Most common without stop-words:\\n',nltk.FreqDist(text_words_clear).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Exercise 4\n",
    "In the processed in the above two exrcises text [\"Cinderella\" fairy tale](https://www.cs.cmu.edu/~spok/grimmtmp/016.txt) remove all stop words as shown above and update the dictionary with words amount. Thus, as before you should obtain another dictionary (call it `cinderella_stop`) with updated keyses and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\tCinderella\\nThe wife of a rich man fell sick, and as she felt that her end\\nwas drawing near, she cal'\n",
      "{'Cinderella': 3, 'The': 16, 'wife': 4, 'rich': 1, 'man': 4, 'fell': 2, 'sick': 1, ',': 260, 'felt': 1, 'end': 1, 'draw': 1, 'near': 2, 'call': 5, 'daughter': 4, 'bedside': 1, 'say': 25, 'dear': 1, 'child': 1, 'good': 8, 'pious': 2, 'God': 1, 'always': 3, 'protect': 1, 'I': 4, 'look': 7, 'heaven': 1, '.': 111, 'Thereupon': 1, 'close': 1, 'eye': 3, 'depart': 1, 'Every': 1, 'day': 7, 'maiden': 12, 'go': 38, 'mother': 6, \"'s\": 20, 'grave': 7, 'wept': 5, 'remain': 3, 'When': 9, 'winter': 1, 'come': 18, 'snow': 1, 'spread': 1, 'white': 6, 'sheet': 1, 'time': 2, 'spring': 1, 'sun': 1, 'drawn': 1, 'take': 14, 'another': 1, 'woman': 1, 'brought': 2, 'house': 3, 'two': 17, 'beautiful': 10, 'fair': 2, 'face': 3, 'vile': 1, 'black': 1, 'heart': 1, 'Now': 1, 'begin': 6, 'bad': 3, 'poor': 1, 'step-child': 1, 'Is': 1, 'stupid': 1, 'goose': 1, 'sit': 4, 'parlor': 1, 'u': 5, 'He': 6, 'want': 5, 'eat': 1, 'bread': 1, 'must': 2, 'earn': 1, 'Out': 1, 'kitchen-wench': 2, 'They': 3, 'pretty': 2, 'clothes': 6, 'away': 8, 'put': 5, 'old': 2, 'grey': 3, 'bedgown': 1, 'give': 7, 'wooden': 2, 'shoe': 18, 'Just': 1, 'proud': 2, 'princess': 2, 'deck': 1, 'cry': 8, 'laugh': 2, 'lead': 1, 'kitchen': 4, 'There': 1, 'hard': 1, 'work': 2, 'morning': 2, 'till': 3, 'night': 1, 'get': 6, 'daybreak': 1, 'carry': 1, 'water': 2, 'light': 1, 'fire': 1, 'cook': 1, 'wash': 2, 'Besides': 1, 'sister': 6, 'every': 2, 'imaginable': 1, 'injury': 1, '-': 7, 'mocked': 1, 'empty': 3, 'pea': 1, 'lentil': 5, 'ash': 10, 'force': 3, 'pick': 21, 'In': 1, 'even': 4, 'weary': 1, 'bed': 1, 'sleep': 1, 'hearth': 1, 'cinder': 1, 'And': 16, 'account': 1, 'dusty': 1, 'dirty': 3, 'cinderella': 26, 'It': 2, 'happen': 2, 'father': 5, 'ask': 2, 'step-daughters': 3, 'bring': 2, 'back': 4, 'Beautiful': 1, 'dress': 10, 'one': 17, 'pearl': 2, 'jewel': 2, 'second': 1, 'Father': 1, 'break': 1, 'first': 2, 'branch': 5, 'knock': 2, 'hat': 2, 'way': 2, 'home': 9, 'So': 2, 'bought': 1, 'rid': 1, 'green': 1, 'thicket': 1, 'hazel': 1, 'twig': 1, 'brush': 2, 'Then': 14, 'broke': 1, 'reach': 1, 'thing': 1, 'wish': 6, 'hazel-bush': 1, 'thanked': 1, \"mother's\": 2, 'plant': 1, 'much': 3, 'tear': 1, 'grow': 1, 'become': 2, 'handsome': 1, 'tree': 9, 'Thrice': 1, 'sat': 3, 'beneath': 6, 'prayed': 1, 'little': 9, 'bird': 11, 'express': 1, 'threw': 4, 'however': 7, 'king': 15, 'order': 2, 'festival': 5, 'last': 3, 'three': 1, 'young': 3, 'girl': 2, 'country': 1, 'invite': 4, 'son': 16, 'might': 3, 'choose': 1, 'bride': 10, 'step-sisters': 3, 'heard': 1, 'appear': 2, 'among': 3, 'number': 1, 'delighted': 2, 'comb': 1, 'hair': 1, 'fasten': 1, 'buckle': 1, 'wedding': 5, 'palace': 1, 'obeyed': 1, 'would': 6, 'like': 3, 'dance': 11, 'beg': 1, 'step-mother': 9, 'allow': 2, 'You': 4, 'cover': 1, 'dust': 1, 'dirt': 2, 'yet': 2, 'As': 3, 'dish': 6, 'picked': 2, 'hour': 4, 'shall': 3, 'back-door': 2, 'garden': 3, 'tame': 2, 'pigeon': 9, 'turtle-doves': 4, 'sky': 4, 'help': 3, 'pot': 2, 'crop': 2, 'window': 1, 'afterwards': 2, 'whir': 2, 'crowd': 2, 'alight': 2, 'amongst': 4, 'nod': 2, 'head': 2, 'rest': 1, 'also': 3, 'gather': 2, 'grain': 1, 'Hardly': 1, 'pass': 3, 'finish': 2, 'flew': 2, 'glad': 2, 'believe': 4, 'But': 6, 'thought': 5, 'certainly': 1, 'kitchen-window': 1, 'length': 1, 'dove': 2, 'others': 2, 'seed': 1, 'half': 1, 'already': 1, 'We': 1, 'ashamed': 1, 'On': 2, 'turn': 9, 'hurry': 1, 'hazel-tree': 7, 'shiver': 3, 'quiver': 3, 'silver': 5, 'gold': 4, 'throw': 3, 'slipper': 5, 'embroider': 1, 'silk': 1, 'She': 5, 'speed': 1, 'Her': 1, 'know': 2, 'foreign': 1, 'golden': 5, 'never': 2, 'prince': 1, 'approach': 1, 'hand': 4, 'let': 1, 'loose': 1, 'else': 1, 'partner': 3, 'bear': 1, 'company': 1, 'see': 2, 'belong': 1, 'escape': 3, 'sprang': 2, 'pigeon-house': 4, 'wait': 5, 'told': 1, 'unknown': 2, 'leapt': 1, 'axe': 2, 'pickaxe': 1, 'hew': 1, 'piece': 1, 'inside': 1, 'lay': 2, 'dim': 1, 'oil-lamp': 1, 'burning': 1, 'mantle-piece': 1, 'jumped': 2, 'quickly': 2, 'run': 2, 'laid': 1, 'seat': 2, 'gown': 2, 'Next': 2, 'afresh': 1, 'parent': 2, 'precede': 1, 'astonish': 1, 'beauty': 1, \"king's\": 2, 'instantly': 1, 'leave': 2, 'follow': 2, 'behind': 2, 'Therein': 1, 'stood': 2, 'tall': 1, 'hung': 1, 'magnificent': 2, 'pear': 1, 'clamber': 1, 'nimbly': 1, 'squirrel': 1, 'climbed': 1, 'pear-tree': 1, 'cut': 5, 'usual': 1, 'side': 2, 'third': 1, 'splendid': 1, 'knew': 1, 'speak': 1, 'astonishment': 1, 'anxious': 1, 'could': 2, 'employ': 1, 'ruse': 1, 'cause': 1, 'whole': 1, 'staircase': 1, 'smear': 1, 'pitch': 1, 'ran': 1, 'left': 5, 'stuck': 1, 'small': 5, 'dainty': 1, 'whose': 1, 'foot': 9, 'fit': 2, 'eldest': 1, 'room': 1, 'try': 1, 'big': 1, 'toe': 4, 'knife': 2, 'queen': 2, 'need': 2, 'swallow': 2, 'pain': 2, 'horse': 5, 'rode': 3, 'oblige': 1, 'pas': 1, 'peep': 6, 'blood': 5, 'within': 2, 'true': 5, 'saw': 2, 'trickle': 1, 'round': 1, 'false': 3, 'chamber': 1, 'safely': 1, 'heel': 3, 'large': 1, 'bit': 2, 'stain': 1, 'stock': 1, 'quite': 1, 'red': 1, 'This': 1, 'right': 4, 'No': 1, 'still': 1, 'stunt': 1, 'late': 1, 'possibly': 1, 'send': 1, 'answer': 1, 'oh': 1, 'show': 1, 'absolutely': 1, 'insist': 1, 'clean': 1, 'bow': 1, 'stool': 1, 'drew': 1, 'heavy': 1, 'glove': 1, 'rise': 1, 'recognize': 1, 'horrify': 1, 'pale': 1, 'rage': 1, 'ride': 1, 'fly': 1, 'place': 1, 'shoulder': 1, 'celebrate': 1, 'favor': 1, 'share': 1, 'fortune': 1, 'betroth': 1, 'couple': 1, 'church': 1, 'elder': 2, 'pecked': 2, 'Afterwards': 1, 'thus': 1, 'wickedness': 1, 'falsehood': 1, 'punish': 1, 'blindness': 1}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "r = requests.get('https://www.cs.cmu.edu/~spok/grimmtmp/016.txt')\n",
    "text = r.text.encode('utf-8')\n",
    "print(text[:100])\n",
    "text = r.text\n",
    "\n",
    "cinderella_lemma = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n",
    "\n",
    "cinderella_stop = [w for w in cinderella_lemma if not(w in stop_words)]\n",
    "\n",
    "cinderella_stop = dict(nltk.FreqDist(cinderella_stop))\n",
    "print(cinderella_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(cinderella_stop, '3f744fff400da23004db2821c96dc2b8b851d1e0', 'Incorrect data', \"Exercise 4 is successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunking, Tagging\n",
    "\n",
    "Used for useful information extracting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging \n",
    "#### Part-of-speech (POS) tagging\n",
    "\n",
    "A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag to each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('part-of-speech', 'JJ'), ('tagger', 'NN'), (',', ','), ('or', 'CC'), ('POS-tagger', 'NNP'), (',', ','), ('processes', 'VBZ'), ('a', 'DT'), ('sequence', 'NN'), ('of', 'IN'), ('words', 'NNS'), (',', ','), ('and', 'CC'), ('attaches', 'VBZ'), ('a', 'DT'), ('part', 'NN'), ('of', 'IN'), ('speech', 'NN'), ('tag', 'NN'), ('to', 'TO'), ('each', 'DT'), ('word', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech\\\n",
    "                            tag to each word\") \n",
    "print (nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a variety of corpora to work with. Corpus reading options\n",
    "Ways to access information for tagged corpora:\n",
    "- .words() - [list of words]\n",
    "- .tagged_words() - [list of (word,tag) pairs]\n",
    "- .sents() - [list of list of words]\n",
    "- .tagged_sents() - [list of list (word,tag) pairs]\n",
    "- .paras() - [list of list of list of words]\n",
    "- .tagged_paras() - [[list of list of list of (word,tag) pairs]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all words from corpus without grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome']\n"
     ]
    }
   ],
   "source": [
    "print (nltk.corpus.gutenberg.words()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all words from corpus group by sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ['CHAPTER', 'I'], ['Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.']]\n"
     ]
    }
   ],
   "source": [
    "print (nltk.corpus.gutenberg.sents()[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all words from corpus group by paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']']], [['VOLUME', 'I']], [['CHAPTER', 'I']], [['Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.']], [['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.'], ['Her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', ';', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', ',', 'who', 'had', 'fallen', 'little', 'short', 'of', 'a', 'mother', 'in', 'affection', '.']]]\n"
     ]
    }
   ],
   "source": [
    "print (nltk.corpus.gutenberg.paras()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cases have similar functions to get a list of words with information about them. Unfortunately not all the available functionality. <br>\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all (word,tag) pairs from corpus without grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "print (nltk.corpus.brown.tagged_words()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all (word,tag) pairs from corpus group by sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print (nltk.corpus.brown.tagged_sents()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all (word,tag) pairs from corpus without by paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "print (nltk.corpus.brown.tagged_paras()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the corps does not have the built-in tools for tagging, you can use the basic method available for any texts or create your advanced. To do this, nltk help to create  <b>Automatic POS tagging</b> and <b>Regular Expression Tagger</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'VB'), ('The', 'VB'), ('King', 'VB'), ('James', 'VB'), ('Bible', 'VB'), (']', 'VB'), ('The', 'VB'), ('Old', 'VB'), ('Testament', 'VB'), ('of', 'VB'), ('the', 'VB'), ('King', 'VB'), ('James', 'VB'), ('Bible', 'VB'), ('The', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "#Automatic POS tagging\n",
    "bible_kjv_words = nltk.word_tokenize(bible_kjv) \n",
    "\n",
    "default_tagger = nltk.DefaultTagger('VB')\n",
    "print (default_tagger.tag(bible_kjv_words)[:15]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'NN'), ('The', 'NN'), ('King', 'VBG'), ('James', 'VBZ'), ('Bible', 'NN'), (']', 'NN'), ('The', 'NN'), ('Old', 'NN'), ('Testament', 'NN'), ('of', 'NN'), ('the', 'NN'), ('King', 'VBG'), ('James', 'VBZ'), ('Bible', 'NN'), ('The', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Regular Expression Tagger\n",
    "patterns = [\n",
    "                (r'.*ing$', 'VBG'),\n",
    "                (r'.*ed$', 'VBD'),\n",
    "                (r'.*es$', 'VBZ'),\n",
    "                (r'.*ould$', 'MD'),\n",
    "                (r'.*\\'s$', 'NN$'),\n",
    "                (r'.*s$', 'NNS'),\n",
    "                (r'.*', 'NN')\n",
    "            ]\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "print (regexp_tagger.tag(bible_kjv_words)[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Tags</h3>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Tag</th>\n",
    "      <th>Meaning</th>\n",
    "      <th>Tag</th>\n",
    "      <th>Meaning</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>ADJ</td>\n",
    "      <td>Adjective</td>\n",
    "      <td>NUM</td>\n",
    "      <td>Number</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>ADV</td>\n",
    "      <td>Adverb</td>\n",
    "      <td>PRO</td>\n",
    "      <td>Pronoun</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>CNJ</td>\n",
    "      <td>Conjunction</td>\n",
    "      <td>P</td>\n",
    "      <td>Preposition</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>DET</td>\n",
    "      <td>Determiner</td>\n",
    "      <td>TO</td>\n",
    "      <td>The word to</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>EX</td>\n",
    "      <td>Existential</td>\n",
    "      <td>UH</td>\n",
    "      <td>Interjection</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>FW</td>\n",
    "      <td>Foreign word</td>\n",
    "      <td>V</td>\n",
    "      <td>Verb</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>MOD</td>\n",
    "      <td>Modal werb</td>\n",
    "      <td>VD</td>\n",
    "      <td>Past tense</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>N</td>\n",
    "      <td>Noun</td>\n",
    "      <td>VG</td>\n",
    "      <td>Present participle</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>NP</td>\n",
    "      <td>Proper noun</td>\n",
    "      <td>VN</td>\n",
    "      <td>Past participle</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>NaN</td>\n",
    "      <td>NaN</td>\n",
    "      <td>WH</td>\n",
    "      <td>Wh determiner</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "<i>\n",
    "NLTK provides documentation for each tag, which can be queried using the tag, e.g. `nltk.help.upenn_tagset('RB')`, or a regular expression, e.g. `nltk.help.upenn_tagset('NN.*')`. Some corpora have README files with tagset documentation, see `nltk.corpus.???.readme()`, substituting in the name of the corpus.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More see here <a href=\"http://www.nltk.org/book/ch05.html#sec-using-a-tagger\">Tagger</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Exercise 5\n",
    "Find in  [\"Cinderella\"](https://www.cs.cmu.edu/~spok/grimmtmp/016.txt) text every word's part of speech. Words must contain only alphabetic characters. Save result to list `cinderella_tag`.<br>\n",
    "\n",
    "><i>Example:</i>\n",
    "><pre>[(u'Cinderella', 'NNP'), (u'The', 'DT'), ... ]</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\tCinderella\\nThe wife of a rich man fell sick, and as she felt that her end\\nwas drawing near, she cal'\n",
      "[('cinderella', 'NN'), ('wife', 'NN'), ('rich', 'NN'), ('man', 'NN'), ('fell', 'NN'), ('sick', 'NN'), ('felt', 'NN'), ('end', 'NN'), ('drawing', 'VBG'), ('near', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "r = requests.get('https://www.cs.cmu.edu/~spok/grimmtmp/016.txt')\n",
    "text = r.text.encode('utf-8')\n",
    "print(text[:100])\n",
    "text = r.text\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[A-Za-z]*')\n",
    "text_words = tokenizer.tokenize(text.lower())\n",
    "\n",
    "text_clear = [w for w in text_words if w not in(stop_words) and w != \"\"]\n",
    "\n",
    "text_words = nltk.word_tokenize(text)\n",
    "\n",
    "patterns = [\n",
    "                (r'.*ing$', 'VBG'),\n",
    "                (r'.*ed$', 'VBD'),\n",
    "                (r'.*es$', 'VBZ'),\n",
    "                (r'.*ould$', 'MD'),\n",
    "                (r'.*\\'s$', 'NN$'),\n",
    "                (r'.*s$', 'NNS'),\n",
    "                (r'.*', 'NN')\n",
    "            ]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "cinderella_tag = regexp_tagger.tag(text_clear)\n",
    "print(cinderella_tag[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(cinderella_tag, '7459ca1577e460b13fd547b9a379159f448264c4', 'Incorrect data', \"Exercise 5 is successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "Chunking is the basic technique for entity recognition, which selects a subset of the tokens.\n",
    "The pieces produced by a chunker do not overlap in the source text. Chunking is useful to identify key words, recognition of entities, dependencies search, ect. \n",
    "<br>\n",
    "Chunking allows us to extract useful information from the text, namely:\n",
    "- keywords\n",
    "- entity\n",
    "- relation\n",
    "- and other applicable areas\n",
    "\n",
    "One type of chunking is Noun Phrase Chunking (NP). It is based on the noun phrase and an individual noun phrase that contains no other NP-chunks. An NP chunk should be formed whenever the chunker\n",
    "finds an optional determiner (DT) followed by any number\n",
    "of adjectives (JJ)\n",
    "and then a noun (NN).\n",
    "\n",
    "Look at the two most popular methods:\n",
    "- using regular expressions\n",
    "- using NLTK train chunk parser\n",
    "\n",
    "Let us proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking with Regular Expressions\n",
    "Regular expressions are a flexible and versatile tool, start with It.<br>\n",
    "The RegexpParser chunker begins with a flat structure in which no tokens are chunked. The chunking rules are applied in turn, successively updating the chunk structure.  \n",
    "First we need to break up the text on the basic high-level structure using a POS tagging, that considered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sents = nltk.corpus.brown.tagged_sents()\n",
    "print (sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the tag a pattern for deriving chunks:\n",
    "Create simple chunk grammar pattern matches an optional determiner or possessive pronoun, zero or more adjectives, then a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grammar_pattern = r'NP: {<DT|PP\\$>?<JJ>*<NN>}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can using RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/AT\n",
      "  Fulton/NP-TL\n",
      "  County/NN-TL\n",
      "  Grand/JJ-TL\n",
      "  Jury/NN-TL\n",
      "  said/VBD\n",
      "  Friday/NR\n",
      "  an/AT\n",
      "  (NP investigation/NN)\n",
      "  of/IN\n",
      "  Atlanta's/NP$\n",
      "  (NP recent/JJ primary/NN)\n",
      "  (NP election/NN)\n",
      "  produced/VBD\n",
      "  ``/``\n",
      "  no/AT\n",
      "  (NP evidence/NN)\n",
      "  ''/''\n",
      "  that/CS\n",
      "  any/DTI\n",
      "  irregularities/NNS\n",
      "  took/VBD\n",
      "  (NP place/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "regChunckers = nltk.RegexpParser(grammar_pattern)\n",
    "print (regChunckers.parse(sents[0]),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More tags pattern:\n",
    "- determiner/possessiv, adjectives and noun: `{<DT|PP\\$>?<JJ>*<NN>}`;\n",
    "- sequens of proper nouns: `{<NNP>+}`;\n",
    "- consecutive nouns: `{<NN><NN>}`;\n",
    "- ect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Rapunzel/NNP\n",
      "  let/VBD\n",
      "  down/RP\n",
      "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"NP: {<DT|PP\\$>?<JJ>*<NN>}\"\n",
    "regexp_parser = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n",
    "print(regexp_parser.parse(sentence),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S words/NN learn/NN engine/NN)\n"
     ]
    }
   ],
   "source": [
    "nouns = [(\"words\", \"NN\"), (\"learn\", \"NN\"), (\"engine\", \"NN\")]\n",
    "\n",
    "regexp_parser = nltk.RegexpParser(\"NP: {<NNP>+}\")\n",
    "print (regexp_parser.parse(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP words/NN learn/NN) engine/NN)\n"
     ]
    }
   ],
   "source": [
    "regexp_parser = nltk.RegexpParser(\"NP: {<NN><NN>}\")\n",
    "print(regexp_parser.parse(nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "\n",
    "1. In Text can regard words one by one. This is unigram; each word is a gram.\n",
    "<pre>\"I\", \"read\", \"a\", \"book\", \"about\", \"the\", \"history\", \"of\",  \"America\"</pre>\n",
    "2. In Text  can regard words two at a time. This is bigram (digram); each two adjacent words create a bigram.\n",
    "<pre>\"I read\", \"read a\", \"a book\", \"book about\", \"about the\", \"the history\", \"history of\", \"of America\"</pre>\n",
    "3. In Text can regard words three at a time. This is trigram; each three adjacent words create a trigram.\n",
    "<pre>\"I read a\", \"read a book\", \"a book about\", \"book about the\", \"about the history\", \"the history of\", \"history of America\"</pre>\n",
    "4.  In Text  can regard words N at a time. This is n-gram; each N adjacent words create a n-gram.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram taggers are based on a simple statistical algorithm: for each token, assign the tag that is most likely for that particular token. A unigram tagger behaves just like a lookup tagger, except there is a more convenient technique for setting it up, called training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load data from brown corpus from category adventure\n",
    "from nltk.corpus import brown\n",
    "brown_sents = brown.tagged_sents(categories='adventure')\n",
    "\n",
    "#calculate split position\n",
    "size = int(len(brown_sents)*0.75)\n",
    "\n",
    "#split data to train and test\n",
    "train_sents = brown_sents[:size]\n",
    "test_sents = brown_sents[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8067872317245888"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "unigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Tagger similar to the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1695173766205469"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "bigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the bigram tagger manages to tag every word in a sentence it saw during training, but does badly on an unseen sentence. As soon as it encounters a new word, it is unable to assign a tag. It cannot tag the following word even if it was seen during training, simply because it never saw it during training with a None tag on the previous word. Consequently, the tagger fails to tag the rest of the sentence. Its overall accuracy score is very low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram chunkers is based on N-gram tagger. How to do it see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  NLTK train chunk parser "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the corpus module we can load Wall Street Journal text that has been tagged then chunked using the IOB notation. The chunk categories provided in this corpus are NP, VP and PP. \n",
    "\n",
    "The CoNLL 2000 corpus contains 270k words of Wall Street Journal text, divided into \"train\" and \"test\" portions, annotated with part-of-speech tags and chunk tags in the IOB format. We can access the data using nltk.corpus.conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('S', [Tree('NP', [('Confidence', 'NN')]), Tree('PP', [('in', 'IN')]), Tree('NP', [('the', 'DT'), ('pound', 'NN')]), Tree('VP', [('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB')]), Tree('NP', [('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN')]), ('if', 'IN'), Tree('NP', [('trade', 'NN'), ('figures', 'NNS')]), Tree('PP', [('for', 'IN')]), Tree('NP', [('September', 'NNP')]), (',', ','), ('due', 'JJ'), Tree('PP', [('for', 'IN')]), Tree('NP', [('release', 'NN')]), Tree('NP', [('tomorrow', 'NN')]), (',', ','), Tree('VP', [('fail', 'VB'), ('to', 'TO'), ('show', 'VB')]), Tree('NP', [('a', 'DT'), ('substantial', 'JJ'), ('improvement', 'NN')]), Tree('PP', [('from', 'IN')]), Tree('NP', [('July', 'NNP'), ('and', 'CC'), ('August', 'NNP')]), Tree('NP', [(\"'s\", 'POS'), ('near-record', 'JJ'), ('deficits', 'NNS')]), ('.', '.')]), Tree('S', [('Chancellor', 'NNP'), Tree('PP', [('of', 'IN')]), Tree('NP', [('the', 'DT'), ('Exchequer', 'NNP')]), Tree('NP', [('Nigel', 'NNP'), ('Lawson', 'NNP')]), Tree('NP', [(\"'s\", 'POS'), ('restated', 'VBN'), ('commitment', 'NN')]), Tree('PP', [('to', 'TO')]), Tree('NP', [('a', 'DT'), ('firm', 'NN'), ('monetary', 'JJ'), ('policy', 'NN')]), Tree('VP', [('has', 'VBZ'), ('helped', 'VBN'), ('to', 'TO'), ('prevent', 'VB')]), Tree('NP', [('a', 'DT'), ('freefall', 'NN')]), Tree('PP', [('in', 'IN')]), Tree('NP', [('sterling', 'NN')]), Tree('PP', [('over', 'IN')]), Tree('NP', [('the', 'DT'), ('past', 'JJ'), ('week', 'NN')]), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "print (conll2000.chunked_sents('train.txt')[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the CoNLL 2000 corpus contains three chunk types: NP chunks, which we have already seen; VP chunks such as has already delivered; and PP chunks such as because of. Since we are only interested in the NP chunks right now, we can use the chunk_types argument to select them. Load train and test data consist from NP chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  in/IN\n",
      "  (NP the/DT pound/NN)\n",
      "  is/VBZ\n",
      "  widely/RB\n",
      "  expected/VBN\n",
      "  to/TO\n",
      "  take/VB\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  if/IN\n",
      "  (NP trade/NN figures/NNS)\n",
      "  for/IN\n",
      "  (NP September/NNP)\n",
      "  ,/,\n",
      "  due/JJ\n",
      "  for/IN\n",
      "  (NP release/NN)\n",
      "  (NP tomorrow/NN)\n",
      "  ,/,\n",
      "  fail/VB\n",
      "  to/TO\n",
      "  show/VB\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  from/IN\n",
      "  (NP July/NNP and/CC August/NNP)\n",
      "  (NP 's/POS near-record/JJ deficits/NNS)\n",
      "  ./.) (S\n",
      "  (NP Rockwell/NNP International/NNP Corp./NNP)\n",
      "  (NP 's/POS Tulsa/NNP unit/NN)\n",
      "  said/VBD\n",
      "  (NP it/PRP)\n",
      "  signed/VBD\n",
      "  (NP a/DT tentative/JJ agreement/NN)\n",
      "  extending/VBG\n",
      "  (NP its/PRP$ contract/NN)\n",
      "  with/IN\n",
      "  (NP Boeing/NNP Co./NNP)\n",
      "  to/TO\n",
      "  provide/VB\n",
      "  (NP structural/JJ parts/NNS)\n",
      "  for/IN\n",
      "  (NP Boeing/NNP)\n",
      "  (NP 's/POS 747/CD jetliners/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "train = conll2000.chunked_sents('train.txt',chunk_types=['NP'])\n",
    "test  = conll2000.chunked_sents('test.txt',chunk_types=['NP'])\n",
    "print (train[0],test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the UnigramChunker class, which uses a unigram tagger to label sentences with chunk tags. Most of the code in this class is simply used to convert back and forth between the chunk tree representation used by NLTK's ChunkParserI interface, and the IOB representation used by the embedded tagger. The class defines two methods: a constructor, which is called when we build a new UnigramChunker; and the parse method, which is used to chunk new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents): \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data) \n",
    "\n",
    "    def parse(self, sentence): \n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor  expects a list of training sentences, which will be in the form of chunk trees. It first converts training data to a form that is suitable for training the tagger, using tree2conlltags to map each chunk tree to a list of word,tag,chunk triples. It then uses that converted training data to train a unigram tagger, and stores it in self.tagger for later use.\n",
    "\n",
    "The parse method takes a tagged sentence as its input, and begins by extracting the part-of-speech tags from that sentence. It then tags the part-of-speech tags with IOB chunk tags, using the tagger self.tagger that was trained in the constructor. Next, it extracts the chunk tags, and combines them with the original sentence, to yield conlltags. Finally, it uses conlltags2tree to convert the result back into a chunk tree.\n",
    "\n",
    "Now that we have UnigramChunker, we can train it using the CoNLL 2000 corpus, and test its resulting performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "unigram_chunker = UnigramChunker(train)\n",
    "print(unigram_chunker.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chunker does reasonably well, achieving an overall f-measure score of 83%. Let's take a look at what it's learned, by using its unigram tagger to assign a tag to each of the part-of-speech tags that appear in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'), (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'), ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'), ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'), ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'), ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'), ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'), ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'), ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'), ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\n"
     ]
    }
   ],
   "source": [
    "postags = sorted(set(pos for sent in train for (word,pos) in sent.leaves()))\n",
    "print(unigram_chunker.tagger.tag(postags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has discovered that most punctuation marks occur outside of NP chunks, with the exception of `#` and `$`, both of which are used as currency markers. It has also found that determiners (`DT`) and possessives (`PRP$` and `WP$`) occur at the beginnings of NP chunks, while noun types (`NN`, `NNP`, `NNPS`, `NNS`) mostly occur inside of NP chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built a unigram chunker, it is quite easy to build a bigram chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Classifier-Based Chunkers\n",
    "\n",
    "\n",
    "Both the regular-expression based chunkers and the n-gram chunkers decide what chunks to create entirely based on part-of-speech tags. One way that we can incorporate information about the content of words is to use a classifier-based tagger to chunk the sentence. \n",
    "\n",
    "The basic code for the classifier-based NP chunker is shown below. It consists of two classes. The first class used history of words. The second class is basically a wrapper around the tagger class that turns it into a chunker. During training, this second class maps the chunk trees in the training corpus into tag sequences; in the parse() method, it converts the tag sequence provided by the tagger back into a chunk tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history) \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.MaxentClassifier.train( \n",
    "            train_set,  trace=0)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI): \n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only piece left to fill in is the feature extractor. We begin by defining a simple feature extractor which just provides the part-of-speech tag of the current token. Using this feature extractor, our classifier-based chunker is very similar to the unigram chunker, as is reflected in its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}\n",
    "chunker = ConsecutiveNPChunker(train)\n",
    "print(chunker.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can improve the function to extract features and is achieved the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More see here <a href=\"http://www.nltk.org/book/ch05.html#sec-using-a-tagger\">Chunking</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Exercise 6\n",
    "In the processed in the above  exrcises text [\"Cinderella\" fairy tale](https://www.cs.cmu.edu/~spok/grimmtmp/016.txt). Find PP -> P NP and save into variable `result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\tCinderella\\nThe wife of a rich man fell sick, and as she felt that her end\\nwas drawing near, she cal'\n",
      "[('cinderella', 'NN'), ('wife', 'NN'), ('rich', 'NN'), ('man', 'NN'), ('fell', 'NN'), ('sick', 'NN'), ('felt', 'NN'), ('end', 'NN'), ('drawing', 'VBG'), ('near', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "r = requests.get('https://www.cs.cmu.edu/~spok/grimmtmp/016.txt')\n",
    "text = r.text.encode('utf-8')\n",
    "print(text[:100])\n",
    "text = r.text\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[A-Za-z]*')\n",
    "text_words = tokenizer.tokenize(text.lower())\n",
    "\n",
    "text_clear = [w for w in text_words if w not in(stop_words) and w != \"\"]\n",
    "\n",
    "text_words = nltk.word_tokenize(text)\n",
    "\n",
    "patterns = [\n",
    "                (r'.*ing$', 'VBG'),\n",
    "                (r'.*ed$', 'VBD'),\n",
    "                (r'.*es$', 'VBZ'),\n",
    "                (r'.*ould$', 'MD'),\n",
    "                (r'.*\\'s$', 'NN$'),\n",
    "                (r'.*s$', 'NNS'),\n",
    "                (r'.*', 'NN')\n",
    "            ]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "cinderella_tag = regexp_tagger.tag(text_clear)\n",
    "\n",
    "grammar = r\"NP: {<DT|PP\\$>?<JJ>*<P.NP>}\"\n",
    "regexp_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "result = regexp_parser.parse(cinderella_tag)\n",
    "print (result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(result, '7ed03dcba6bc5b16970ba19b517b18d0c34d5324', 'Incorrect data', \"Exercise 6 is successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Presented by <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"http://datascience-school.com\">datascience-school.com</a></h3></center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
